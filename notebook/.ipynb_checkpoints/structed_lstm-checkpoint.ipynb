{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lines = open('/Users/Alan/Nuance/zh-parse-tool/corpus/TrainSet-eCarX-171019.txt', encoding='gbk').readlines()\n",
    "# labels = [i.split()[0] for i in lines]\n",
    "# labels = list(set(labels))\n",
    "# label_map = np.load('./multi_label_map')\n",
    "# def combine(data, key, index, labels):\n",
    "#     string = []\n",
    "#     for i in range(5):\n",
    "#         temp = data[i][key][index]\n",
    "#         string.append(label_map[i][np.argmax(temp)])\n",
    "#     string = [i for i in string if i != 'NULL']\n",
    "#     rebuild = '.'.join(string)\n",
    "#     result = np.zeros((len(labels)))\n",
    "#     result[labels.index(rebuild)] = 1.0\n",
    "#     return result\n",
    "\n",
    "# combined_targets = dict()\n",
    "# combined_train_targets = dict()\n",
    "# combined_valid_targets = dict()\n",
    "\n",
    "# keyname = 'train_targets'\n",
    "# keys = data_all[keyname][0].keys()\n",
    "# for key in keys:\n",
    "#     size = len(data_all[keyname][0][key])\n",
    "#     combined_train_targets[key] = np.zeros((size, len(labels)))\n",
    "#     for i in range(size):\n",
    "#         combined_train_targets[key][i] = combine(data_all[keyname], key, 0, labels)\n",
    "        \n",
    "# keyname = 'valid_targets'\n",
    "# keys = data_all[keyname][0].keys()\n",
    "# for key in keys:\n",
    "#     size = len(data_all[keyname][0][key])\n",
    "#     combined_valid_targets[key] = np.zeros((size, len(labels)))\n",
    "#     for i in range(size):\n",
    "#         combined_valid_targets[key][i] = combine(data_all[keyname], key, 0, labels)\n",
    "        \n",
    "# combined_targets['train_targets'] = combined_train_targets\n",
    "# combined_targets['valid_targets'] = combined_valid_targets\n",
    "# pickle.dump(combined_targets, open('./rebuild_label.npz', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pickle.load(open('./multi_train_data', 'rb'))\n",
    "label_all = pickle.load(open('./multi_train_label', 'rb'))\n",
    "test_data = pickle.load(open('./multi_test_data', 'rb'))\n",
    "test_label_all = pickle.load(open('./multi_test_label', 'rb'))\n",
    "sizes = pickle.load(open('./multi_label_sizes', 'rb'))\n",
    "rebuild_label = pickle.load(open('./rebuild_label.npz', 'rb'))\n",
    "\n",
    "data_all = {\n",
    "    'train_inputs': data,\n",
    "    'train_targets': label_all,\n",
    "    'valid_inputs': test_data,\n",
    "    'valid_targets': test_label_all,\n",
    "    'label_size': sizes\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class parameter(object):\n",
    "    \n",
    "    def __init__(self, shape):\n",
    "        self.shape = shape\n",
    "    \n",
    "    def normal(self):\n",
    "        return tf.Variable(tf.random_normal(self.shape))\n",
    "\n",
    "class layer(object):\n",
    "    \n",
    "    def __init__(self, shape):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def __str__(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    \n",
    "    \n",
    "class lstm_layer(layer):\n",
    "    \n",
    "    def __init__(self, inputs, n_units):\n",
    "        \n",
    "        self.n_units = n_units\n",
    "        self.input_shape = inputs.shape\n",
    "        self.outputs, self.states = tf.nn.dynamic_rnn(\n",
    "            cell = tf.contrib.rnn.BasicLSTMCell(self.n_units),\n",
    "            inputs = inputs,\n",
    "            dtype = tf.float32)\n",
    "        \n",
    "    def __str__(self):\n",
    "        print('|---------------[LSTM layer]---------------|')\n",
    "        print('shape: ', self.input_shape, ' => ', self.outputs.shape)\n",
    "        return '|_______________[LSTM layer]_______________|'\n",
    "        \n",
    "class affine_layer(layer):\n",
    "    \n",
    "    def __init__(self, inputs, shape, activation):\n",
    "        \n",
    "        self.shape = shape\n",
    "        self.input_shape = inputs.shape\n",
    "        self.activation = activation\n",
    "        self.weights = parameter(self.shape).normal()\n",
    "        self.biases = parameter([self.shape[-1]]).normal()\n",
    "        self.outputs = self.activation(tf.add(tf.matmul(inputs, self.weights), self.biases))\n",
    "\n",
    "    def __str__(self):\n",
    "        print('|---------------[Affine layer]---------------|')\n",
    "        print('shape: ', self.input_shape, ' => ', self.outputs.shape)\n",
    "        return '|_______________[Affine layer]_______________|'\n",
    "    \n",
    "class reshape_layer(layer):\n",
    "    \n",
    "    def __init__(self, inputs, shape):\n",
    "        \n",
    "        self.input_shape = inputs.shape\n",
    "        self.outputs = tf.reshape(inputs, shape)\n",
    "\n",
    "    def __str__(self):\n",
    "        print('|---------------[Reshape layer]---------------|')\n",
    "        print('shape: ', self.input_shape, ' => ', self.outputs.shape)\n",
    "        return '|_______________[Reshape layer]_______________|'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class lstm_model(object):\n",
    "    \n",
    "    def __init__(self, data, model_id, n_affine = 1):\n",
    "        \n",
    "        self.frame_size = 16\n",
    "        self.learning_rate = 1e-3\n",
    "        self.data = data\n",
    "        self.output_size = int(self.data['label_size'][model_id])\n",
    "        self.hidden_unit1 = 256\n",
    "#         self.graph = graph_name\n",
    "#         print(self.output_size)\n",
    "        \n",
    "        \n",
    "        with graph.as_default():\n",
    "            \n",
    "            with tf.variable_scope('single_model_'+str(model_id)):\n",
    "                self.input_placeholder = tf.placeholder(tf.float32, [None, None, self.frame_size])\n",
    "                self.target_placeholder = tf.placeholder(tf.float32, [None, self.output_size])\n",
    "                self.batch_size_placeholder = tf.placeholder(tf.int32)\n",
    "\n",
    "                self.layers = []\n",
    "                self.layers.append(lstm_layer(self.input_placeholder, self.frame_size))\n",
    "                self.layers.append(reshape_layer(self.layers[-1].outputs[:,-1,:], [-1, self.frame_size]))\n",
    "    #             print(self.layers[-1].outputs)\n",
    "                self.layers.append(affine_layer(self.layers[-1].outputs, [self.frame_size, self.hidden_unit1], tf.nn.relu))\n",
    "                self.layers.append(affine_layer(self.layers[-1].outputs, [self.hidden_unit1, self.output_size], tf.identity))\n",
    "\n",
    "                self.loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=self.layers[-1].outputs, labels=self.target_placeholder))\n",
    "                self.optimizer = tf.train.AdamOptimizer\n",
    "                self.training = self.optimizer(self.learning_rate).minimize(self.loss)\n",
    "                self.accuracy = tf.reduce_sum(tf.cast(tf.equal(tf.argmax(self.layers[-1].outputs, 1), tf.argmax(self.target_placeholder, 1)), tf.float32))\n",
    "    #             self.number = self.target_placeholder.shape[0]\n",
    "                \n",
    "                self.sess = tf.Session()\n",
    "                self.sess.run(tf.global_variables_initializer())\n",
    "#             print(self.number)\n",
    "            \n",
    "    def train(self, iteration = 100, interval = 5):\n",
    "           \n",
    "        with graph.as_default():\n",
    "            with tf.variable_scope('single_model_'+str(model_id)):\n",
    "                for i in range(iteration):\n",
    "                    accs = 0.0\n",
    "                    errs = 0.0\n",
    "                    ns = 0\n",
    "                    for key, j in zip(self.data['train_inputs'], range(len(self.data['train_inputs']))):\n",
    "                        train_inputs = np.array(self.data['train_inputs'][key])\n",
    "                        train_targets = np.array(self.data['train_targets'][key])\n",
    "                        batch_size = key\n",
    "                        feed_dict = {self.input_placeholder: train_inputs, self.target_placeholder: train_targets, self.batch_size_placeholder: batch_size}\n",
    "                        _, acc, err = self.sess.run([self.training, self.accuracy, self.loss], feed_dict = feed_dict)\n",
    "                        accs += acc\n",
    "                        errs += err\n",
    "                        ns += train_inputs.shape[0]\n",
    "                    accs /= ns\n",
    "                    errs /= ns\n",
    "                    print('Training Epoch: %d, acc: %f, loss: %f '%(i, accs, errs))\n",
    "                if i % interval:\n",
    "                    accs = 0.0\n",
    "                    errs = 0.0\n",
    "                    ns = 0\n",
    "                    for key, j in zip(self.data['valid_inputs'], range(len(self.data['valid_inputs']))):\n",
    "                        valid_inputs = np.array(self.data['valid_inputs'][key])\n",
    "                        valid_targets = np.array(self.data['valid_targets'][key])\n",
    "                        batch_size = key\n",
    "                        feed_dict = {self.input_placeholder: valid_inputs, self.target_placeholder: valid_targets, self.batch_size_placeholder: batch_size}\n",
    "                        _, acc, err = self.sess.run([self.training, self.accuracy, self.loss], feed_dict = feed_dict)\n",
    "                        accs += acc\n",
    "                        errs += err\n",
    "                        ns += train_inputs.shape[0]\n",
    "                    accs /= ns\n",
    "                    errs /= ns\n",
    "                    print('------------------------------------------------------')\n",
    "                    print('Validation Epoch: %d, acc: %f, loss: %f '%(i, accs, errs))\n",
    "                    print('------------------------------------------------------')\n",
    "        \n",
    "    def record(self):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def __str__(self):\n",
    "        for layer in self.layers:\n",
    "            print(layer)\n",
    "        return ''\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# class multi_model(object):\n",
    "    \n",
    "#     def __init__(self, data, rebuild_target):\n",
    "        \n",
    "#         self.data = data\n",
    "#         self.ids = np.arange(5)\n",
    "#         self.data = dict()\n",
    "#         self.graph = tf.Graph()\n",
    "#         self.models = dict()\n",
    "#         self.output_size = 30\n",
    "#         self.hidden_unit1 = 128\n",
    "#         self.learning_rate = 1e-3\n",
    "#         self.rebuild_target = rebuild_target\n",
    "#         for i in self.ids:\n",
    "#             self.data[i] = {'train_inputs': data['train_inputs'], 'train_targets': data['train_targets'][i], 'label_size': data['label_size']}\n",
    "#             self.models[i] = lstm_model(data=self.data[i], model_id = i)\n",
    "#         self.separated_output = [i.layers[-1].outputs for i in self.models.values()]\n",
    "#         self.inputs = tf.concat(self.separated_output, 1)\n",
    "# #         print(self.inputs)\n",
    "# #         self.graph = graph\n",
    "        \n",
    "#         with graph.as_default():\n",
    "#             with tf.name_scope('multi_model'):\n",
    "#                 self.rebuild_target_placeholder = tf.placeholder(tf.float32, [None, 134])\n",
    "\n",
    "#                 self.layers = []\n",
    "#                 self.layers.append(affine_layer(self.inputs, [135, self.hidden_unit1], tf.nn.relu))\n",
    "#                 self.layers.append(affine_layer(self.layers[-1].outputs, [self.hidden_unit1, 134], tf.nn.relu))\n",
    "\n",
    "#                 self.loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=self.layers[-1].outputs, labels=self.rebuild_target_placeholder))\n",
    "#                 self.optimizer = tf.train.AdamOptimizer\n",
    "#                 self.training = self.optimizer(self.learning_rate).minimize(self.loss)\n",
    "#                 self.accuracy = tf.reduce_sum(tf.cast(tf.equal(tf.argmax(self.layers[-1].outputs, 1), tf.argmax(self.rebuild_target_placeholder, 1)), tf.float32))\n",
    "\n",
    "#                 self.sess = tf.Session()\n",
    "#                 merged = tf.summary.merge_all()\n",
    "#                 writer = tf.summary.FileWriter('./output',self.sess.graph) \n",
    "#                 self.sess.run(tf.global_variables_initializer())\n",
    "            \n",
    "        \n",
    "#     def train(self, iteration = 100, interval = 5):\n",
    "        \n",
    "#         with graph.as_default():\n",
    "#             with tf.name_scope('multi_model'):\n",
    "#             for i in range(iteration):\n",
    "#                 accs = 0.0\n",
    "#                 errs = 0.0\n",
    "#                 ns = 0\n",
    "#                 for key, j in zip(self.data[0]['train_inputs'], range(len(self.data[0]['train_inputs']))):\n",
    "#                     train_inputs = np.array(self.data[0]['train_inputs'][key])\n",
    "#                     train_targets = np.array(self.rebuild_target['train_targets'][key])\n",
    "#                     batch_size = key\n",
    "#                     feed_dict = {self.rebuild_target_placeholder: np.array(train_targets, dtype=np.float32)}\n",
    "#                     _, acc, err = self.sess.run([self.training, self.accuracy, self.loss], feed_dict = feed_dict)\n",
    "#                     accs += acc\n",
    "#                     errs += err\n",
    "#                     ns += train_inputs.shape[0]\n",
    "#                 accs /= ns\n",
    "#                 errs /= ns\n",
    "#                 print('Training Epoch: %d, acc: %f, loss: %f '%(i, accs, errs))\n",
    "#             if i % interval:\n",
    "#                 accs = 0.0\n",
    "#                 errs = 0.0\n",
    "#                 ns = 0\n",
    "#                 for key, j in zip(self.data[0]['valid_inputs'], range(len(self.data[0]['valid_inputs']))):\n",
    "#                     valid_inputs = np.array(self.data[0]['valid_inputs'][key])\n",
    "#                     valid_targets = np.array(self.rebuild_target['valid_targets'][key])\n",
    "#                     batch_size = key\n",
    "#                     feed_dict = {self.rebuild_target_placeholder: np.array(train_targets, dtype=np.float32)}\n",
    "#                     _, acc, err = self.sess.run([self.training, self.accuracy, self.loss], feed_dict = feed_dict)\n",
    "#                     accs += acc\n",
    "#                     errs += err\n",
    "#                     ns += train_inputs.shape[0]\n",
    "#                 accs /= ns\n",
    "#                 errs /= ns\n",
    "#                 print('------------------------------------------------------')\n",
    "#                 print('Validation Epoch: %d, acc: %f, loss: %f '%(i, accs, errs))\n",
    "#                 print('------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# id = 0\n",
    "# data_all = {'train_inputs': data, 'train_targets': label_all[id], 'label_size': sizes}\n",
    "# graph = tf.Graph()\n",
    "# model1 = lstm_model(data=data_all, model_id=id, n_affine=1, graph_name = graph)\n",
    "# model1.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
