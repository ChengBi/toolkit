{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pickle.load(open('../word2vec/anouymous_train_data_200.npz', 'rb'))\n",
    "train_inputs = train_data['inputs']\n",
    "train_targets = train_data['targets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic_lstm_layer(inputs, n_units):\n",
    "    \n",
    "    shape = inputs.shape.as_list()\n",
    "    batch_size = shape[0]\n",
    "    step_size = shape[1]\n",
    "    n_units = shape[2]\n",
    "    \n",
    "    def param_init(name, shape):\n",
    "        return tf.Variable(name = name, initial_value = tf.truncated_normal(shape), dtype = tf.float32)\n",
    "    \n",
    "#     def initial_state(batch_size, n_units):\n",
    "#         h = tf.Variable(tf.zeros(batch_size, n_units), dtype = tf.float32)\n",
    "#         c = tf.Variable(tf.zeros(n_units), dtype = tf.float32)\n",
    "#         return h,c\n",
    "        \n",
    "    def BasicLSTMCell(n_units):\n",
    "        W = dict()\n",
    "        B = dict()\n",
    "        c = tf.Variable(tf.zeros([n_units]), dtype = tf.float32)\n",
    "        h = tf.reshape(inputs[:,0,:] - inputs[:,0,:], [-1, 1, n_units])\n",
    "#         h.append(h[-1])\n",
    "        wf = param_init('wf', [n_units, n_units])\n",
    "        bf = param_init('bf', [n_units])\n",
    "        wi = param_init('wi', [n_units, n_units])\n",
    "        bi = param_init('bi', [n_units])\n",
    "        wc = param_init('wc', [n_units, n_units])\n",
    "        bc = param_init('bc', [n_units])\n",
    "        wo = param_init('wo', [n_units, n_units])\n",
    "        bo = param_init('bo', [n_units])\n",
    "        W = tf.concat([wf, wi, wc, wo], 0)\n",
    "        B = tf.concat([bf, bi, bc, bo], 0)\n",
    "        return W, B, c, h\n",
    "    \n",
    "    def train_step(cell, xt):\n",
    "        \n",
    "        V = tf.transpose(tf.matmul(cell[0], tf.transpose(xt, [1, 0])) + tf.matmul(cell[0], tf.transpose(cell[3][-1], [1,0])), [1,0]) + cell[1]\n",
    "        ft, it, c, ot = tf.split(V, num_or_size_splits=4, axis=1)\n",
    "        ft = tf.sigmoid(ft)\n",
    "        it = tf.sigmoid(it)\n",
    "        c = tf.tanh(c)\n",
    "        ot = tf.sigmoid(ot)\n",
    "        cell[2] = tf.reshape(tf.reduce_mean(ft * tf.tile(cell[2], [batch_size, 1]) + it * c, 0), [1, n_units])\n",
    "        cell[3] = tf.concat([cell[3], ot * tf.tile(tf.tanh(cell[2]), [batch_size, 1])], 1)\n",
    "        return cell\n",
    "        \n",
    "    def iteration(cell, inputs):\n",
    "        if step_size == None:\n",
    "            return cell[2], cell[3]\n",
    "        for i in range(step_size):\n",
    "            xt = inputs[:, i, :]\n",
    "            cell= train_step(cell, xt)\n",
    "        return cell[2], cell[3]\n",
    "        \n",
    "\n",
    "    cell = BasicLSTMCell(n_units)\n",
    "    print(cell)\n",
    "    if batch_size != None:\n",
    "        ht = tf.Variable(tf.zeros(batch_size, n_units))\n",
    "    ht = tf.Variable(tf.zeros([1, n_units]))\n",
    "    states, outputs = iteration(cell, inputs)\n",
    "    return states, outputs\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor 'concat:0' shape=(800, 200) dtype=float32>, <tf.Tensor 'concat_1:0' shape=(800,) dtype=float32>, <tf.Variable 'Variable:0' shape=(200,) dtype=float32_ref>, <tf.Tensor 'Reshape:0' shape=(?, 1, 200) dtype=float32>)\n",
      "Tensor(\"Reshape:0\", shape=(?, 1, 200), dtype=float32)\n",
      "Tensor(\"Reshape:0\", shape=(?, 1, 200), dtype=float32)\n",
      "|Training Epoch0 | ACC: 0.007463 | ERR: 92.882615 | SIZE: 134000 |\n",
      "|Training Epoch1 | ACC: 0.007463 | ERR: 92.882615 | SIZE: 268000 |\n",
      "|Training Epoch2 | ACC: 0.007463 | ERR: 92.882615 | SIZE: 402000 |\n",
      "|Training Epoch3 | ACC: 0.007463 | ERR: 92.882615 | SIZE: 536000 |\n",
      "|Training Epoch4 | ACC: 0.007463 | ERR: 92.882615 | SIZE: 670000 |\n",
      "|Training Epoch5 | ACC: 0.007463 | ERR: 92.882615 | SIZE: 804000 |\n",
      "|Training Epoch6 | ACC: 0.007463 | ERR: 92.882615 | SIZE: 938000 |\n"
     ]
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    n_units = 200\n",
    "    n_classes = 134\n",
    "    n_hidden = 128\n",
    "    \n",
    "    inputs_placeholder = tf.placeholder(tf.float32, [None, None, n_units])\n",
    "    targets_placeholder = tf.placeholder(tf.float32, [None, n_classes])\n",
    "    \n",
    "    states, outputs = dynamic_lstm_layer(inputs_placeholder, n_units)\n",
    "    print(outputs)\n",
    "    \n",
    "    w1 = tf.Variable(tf.truncated_normal([n_units, n_units]), tf.float32)\n",
    "    b1 = tf.Variable(tf.zeros([n_units]))\n",
    "    w2 = tf.Variable(tf.truncated_normal([n_units, n_classes]), tf.float32)\n",
    "    b2 = tf.Variable(tf.zeros([n_classes]))\n",
    "    print(outputs)\n",
    "    l1 = tf.nn.relu(tf.add(tf.matmul(outputs[:, -1, :], w1), b1))\n",
    "    o1 = tf.nn.relu(tf.add(tf.matmul(l1, w2), b2))\n",
    "    \n",
    "    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=o1, labels=targets_placeholder))\n",
    "    loss_sum = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(logits=o1, labels=targets_placeholder))\n",
    "    accuracy = tf.reduce_sum(tf.cast(tf.equal(tf.argmax(o1, 1), tf.argmax(targets_placeholder, 1)), tf.float32))\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(loss)\n",
    "    \n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    train_keys = train_inputs.keys()\n",
    "    for i in range(100):\n",
    "        accs = 0.0\n",
    "        errs = 0.0\n",
    "        n = 0\n",
    "        for key in train_keys:\n",
    "            batch_inputs = train_inputs[key]\n",
    "            batch_targets = train_targets[key]\n",
    "            size = len(batch_inputs)\n",
    "#             print(key)\n",
    "            if size == 0:\n",
    "                continue\n",
    "            feed_dict = {inputs_placeholder:batch_inputs, targets_placeholder:batch_targets}\n",
    "            _, acc, err = sess.run([optimizer, accuracy, loss_sum], feed_dict = feed_dict)\n",
    "            accs += acc\n",
    "            errs += err\n",
    "            n += size\n",
    "        print('|Training Epoch%d | ACC: %f | ERR: %f | SIZE: %d |'%(i, accs/n, errs/n, n))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"concat_2:0\", shape=(50, 2, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "a = tf.zeros([50,1, 10])\n",
    "print(tf.concat([a, a], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
