{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cheng.bi\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:862: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import re\n",
    "from tqdm import tqdm, tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class corpusClean():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.num_re = re.compile('[0-9一二三四五六七八九十]+')\n",
    "        self.char_re = re.compile('[a-zA-Z\\-\\,\\.\\。\\，]+')\n",
    "\n",
    "    def clean(self, line):\n",
    "        raw_words = [i for i in line]\n",
    "#         print(raw_words)\n",
    "        new_words = []\n",
    "        for i in range(len(raw_words)):\n",
    "            word = raw_words[i]\n",
    "            if 65281 <= ord(word) and ord(word) <= 65375:\n",
    "                word = chr(ord(word)-65248)\n",
    "            if (33 <= ord(word) and ord(word) < 48) or (8208 <= ord(word) and ord(word) <= 8232):\n",
    "                word = ''\n",
    "            new_words.append(word)\n",
    "        sentence = ''.join(new_words)\n",
    "        \n",
    "        removed_char = re.sub(self.char_re, ' <CHAR> ', sentence)\n",
    "        removed_num = re.sub(self.num_re, ' <NUMBER> ', removed_char)\n",
    "        words = removed_num.split()\n",
    "        saved_words = ['<BEGIN>']\n",
    "        for word in words:\n",
    "            if len(word) == 1 or word.find('<') != -1:\n",
    "                saved_words.append(word)\n",
    "            else:\n",
    "                for w in word:\n",
    "                    saved_words.append(w)\n",
    "        saved_words.append('<END>')\n",
    "        return saved_words\n",
    "    \n",
    "\n",
    "    \n",
    "class layer(object):\n",
    "    def __init__(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def __str__(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "class affine_layer(layer):\n",
    "    def __init__(self, name, inputs, shape, activation, reuse=False):\n",
    "        with tf.variable_scope('affine', reuse=reuse):\n",
    "            self.weights = tf.get_variable(name = name+'_weights', initializer=tf.truncated_normal(shape, stddev=0.05), dtype=tf.float32)\n",
    "            self.bias = tf.get_variable(name = name+'_bias', initializer=tf.zeros([shape[-1]]))\n",
    "            self.outputs = activation(tf.add(tf.matmul(inputs, self.weights), self.bias))\n",
    "    def __str__(self):\n",
    "        return self.outputs.__str__()\n",
    "    \n",
    "class conv_layer(layer):\n",
    "    def __init__(self, name, inputs, shape, activation, reuse=False):\n",
    "        with tf.variable_scope('conv', reuse=reuse):\n",
    "            self.kernel = tf.get_variable(name = name+'_kernel', initializer=tf.truncated_normal(shape, stddev=0.05), dtype=tf.float32)\n",
    "            self.bias = tf.get_variable(name = name+'_bias', initializer=tf.zeros([shape[-1]]))\n",
    "            self.outputs = activation(tf.add(tf.nn.conv2d(inputs, self.kernel, padding='VALID', strides=[1,1,1,1]), self.bias))\n",
    "    def __str__(self):\n",
    "        return self.outputs.__str__()\n",
    "    \n",
    "class pooling_layer(layer):\n",
    "    def __init__(self, name, inputs, reuse=False):\n",
    "        with tf.variable_scope('pooling', reuse=reuse):\n",
    "            self.outputs = tf.nn.max_pool(name = name+'_maxpooling', value=inputs, ksize=[1,2,2,1], padding='VALID', strides=[1,2,2,1])\n",
    "    def __str__(self):\n",
    "        return self.outputs.__str__()\n",
    "        \n",
    "class reshape_layer(layer):\n",
    "    def __init__(self, name, inputs, shape, reuse=False):\n",
    "        with tf.variable_scope('reshape', reuse=reuse):\n",
    "            self.outputs = tf.reshape(name=name, tensor=inputs, shape=shape)\n",
    "    def __str__(self):\n",
    "        return self.outputs.__str__()\n",
    "        \n",
    "class deconv_layer(layer):\n",
    "    def __init__(self, name, inputs, kernel_shape, output_shape, activation, reuse=False):\n",
    "        with tf.variable_scope('deconv', reuse=reuse):\n",
    "            self.kernel = tf.get_variable(name = name+'_kernel', initializer=tf.truncated_normal(kernel_shape, stddev=0.05), dtype=tf.float32)\n",
    "            self.bias = tf.get_variable(name = name+'_bias', initializer=tf.zeros([kernel_shape[-2]]))\n",
    "            self.outputs = activation(tf.add(tf.nn.conv2d_transpose(inputs, self.kernel, output_shape=output_shape, padding='VALID', strides=[1,2,2,1]), self.bias))\n",
    "    def __str__(self):\n",
    "        return self.outputs.__str__()\n",
    "    \n",
    "class lstm_layer(layer):\n",
    "    def __init__(self, name, inputs, n_units, reuse=False):\n",
    "        with tf.variable_scope('lstm', reuse=reuse):\n",
    "            self.outputs, self.states = tf.nn.dynamic_rnn(\n",
    "                cell = tf.contrib.rnn.BasicLSTMCell(n_units),\n",
    "                inputs = inputs,\n",
    "                dtype = tf.float32)\n",
    "    def __str__(self):\n",
    "        return self.outputs.__str__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = open('../data/cleaned_corpus.txt', encoding='utf-8').readlines()\n",
    "lines = open('../data/TrainSet-eCarX-171019.txt', encoding='gbk').readlines()\n",
    "vector_size = 200\n",
    "ngram = 3\n",
    "min_count = 2\n",
    "workers = 4\n",
    "# models = gensim.models.Word2Vec.load('../word2vec/word2vec.model')\n",
    "models = gensim.models.Word2Vec(sentences, size=vector_size, window=ngram, min_count=min_count, workers=workers)\n",
    "cleaner = corpusClean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models.save('../word2vec/word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(train_data, open('../word2vec/train_inputs.npz', 'wb'))\n",
    "pickle.dump(train_target_one_hot, open('../word2vec/train_targets.npz', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = pickle.load(open('../word2vec/train_inputs.npz', 'rb'))\n",
    "train_target_one_hot = pickle.load(open('../word2vec/train_targets.npz', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batched_inputs = dict()\n",
    "batched_targets = dict()\n",
    "for key in train_data.keys():\n",
    "    count = int(len(train_data[key])/50)+1\n",
    "    for i in range(count):\n",
    "        new_key = str(key)+'_'+str(i)\n",
    "        if i+1 != count:\n",
    "            batched_inputs[new_key] = train_data[key][i*50:(i+1)*50]\n",
    "            batched_targets[new_key] = train_target_one_hot[key][i*50:(i+1)*50]\n",
    "        else:\n",
    "            batched_inputs[new_key] = train_data[key][i*50:]\n",
    "            batched_targets[new_key] = train_target_one_hot[key][i*50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pickle.dump(batched_inputs, open('../word2vec/batched_inputs.npz', 'wb'))\n",
    "# pickle.dump(batched_targets, open('../word2vec/batched_targets.npz', 'wb'))\n",
    "batched_inputs = pickle.load(open('../word2vec/batched_inputs.npz', 'rb'))\n",
    "batched_targets = pickle.load(open('../word2vec/batched_targets.npz', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Training Epoch 0, acc: 0.004216, err: nan, n: 134000\n",
      "#Training Epoch 1, acc: 0.013530, err: nan, n: 134000\n",
      "#Training Epoch 2, acc: 0.030985, err: nan, n: 134000\n",
      "#Training Epoch 3, acc: 0.063888, err: nan, n: 134000\n",
      "#Training Epoch 4, acc: 0.090015, err: nan, n: 134000\n",
      "#Training Epoch 5, acc: 0.132321, err: nan, n: 134000\n",
      "#Training Epoch 6, acc: 0.174157, err: nan, n: 134000\n",
      "#Training Epoch 7, acc: 0.207254, err: nan, n: 134000\n",
      "#Training Epoch 8, acc: 0.245701, err: nan, n: 134000\n",
      "#Training Epoch 9, acc: 0.284731, err: nan, n: 134000\n",
      "#Training Epoch 10, acc: 0.327642, err: nan, n: 134000\n",
      "#Training Epoch 11, acc: 0.350366, err: nan, n: 134000\n",
      "#Training Epoch 12, acc: 0.376813, err: nan, n: 134000\n",
      "#Training Epoch 13, acc: 0.409552, err: nan, n: 134000\n",
      "#Training Epoch 14, acc: 0.433149, err: nan, n: 134000\n",
      "#Training Epoch 15, acc: 0.458679, err: nan, n: 134000\n",
      "#Training Epoch 16, acc: 0.474097, err: nan, n: 134000\n",
      "#Training Epoch 17, acc: 0.501836, err: nan, n: 134000\n"
     ]
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    input_placeholder = tf.placeholder(tf.float32, [None, None, 200])\n",
    "    target_placeholder = tf.placeholder(tf.float32, [None, 134])\n",
    "    \n",
    "    lstm_layer1 = lstm_layer('lstm1', input_placeholder, 200)\n",
    "    affine1 = affine_layer('affine1', lstm_layer1.outputs[:,-1,:], [200, 256], tf.nn.relu)\n",
    "    affine2 = affine_layer('affine2', affine1.outputs, [256, 256], tf.nn.relu)\n",
    "    affine3 = affine_layer('affine3', affine2.outputs, [256, 134], tf.identity)\n",
    "    \n",
    "    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=affine3.outputs, labels=target_placeholder))\n",
    "    optimizer = tf.train.AdamOptimizer(1e-3).minimize(loss)\n",
    "    accuracy = tf.reduce_sum(tf.cast(tf.equal(tf.argmax(affine3.outputs, 1), tf.argmax(target_placeholder, 1)), tf.float32))\n",
    "    \n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for i in range(500):\n",
    "        \n",
    "        accs = 0.0\n",
    "        errs = 0.0\n",
    "        n = 0\n",
    "        for key in batched_inputs.keys():\n",
    "            inputs_batch = batched_inputs[key]\n",
    "            targets_batch = batched_targets[key]\n",
    "#             print(inputs_batch.shape)\n",
    "#             print(targets_batch.shape)\n",
    "            feed_dict = {input_placeholder:inputs_batch, target_placeholder:targets_batch}\n",
    "            _, acc, err = sess.run([optimizer, accuracy, loss], feed_dict=feed_dict)\n",
    "            accs += acc\n",
    "            errs += err\n",
    "            n += len(inputs_batch)\n",
    "        print('#Training Epoch %d, acc: %f, err: %f, n: %d'%(i, accs/n, errs/n, n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use raw corpus to generate word vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vector_size = 200\n",
    "ngram = 3\n",
    "min_count = 2\n",
    "workers = 4\n",
    "lines = open('../data/cleaned_corpus.txt', encoding='utf-8').readlines()\n",
    "sentences = [i.split() for i in lines]\n",
    "# print(sentences[0])\n",
    "models = gensim.models.Word2Vec(sentences, size=vector_size, window=ngram, min_count=min_count, workers=workers)\n",
    "models.save('../word2vec/word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134\n",
      "Feature Extraction for training data finished.\n",
      "SUCCESS!\n",
      "Batch data extraction finished.\n",
      "Feature Extraction for validation data finished.\n",
      "SUCCESS!\n",
      "SUCCESS!\n",
      "SUCCESS!\n",
      "Batch data extraction finished.\n"
     ]
    }
   ],
   "source": [
    "lines = open('../data/TrainSet-eCarX-171019.txt').readlines()\n",
    "target_set = set()\n",
    "for line in lines:\n",
    "    target = line.split()[0]\n",
    "    target_set.add(target)\n",
    "target_set = list(target_set)\n",
    "print(len(target_set))\n",
    "models = gensim.models.Word2Vec.load('../word2vec/word2vec.model')\n",
    "cleaner = corpusClean()\n",
    "data = dict()\n",
    "data['train_inputs'] = dict()\n",
    "data['train_targets'] = dict()\n",
    "data['valid_inputs'] = dict()\n",
    "data['valid_targets'] = dict()\n",
    "data['label_map'] = target_set\n",
    "data['batched_train_inputs'] = dict()\n",
    "data['batched_train_targets'] = dict()\n",
    "data['batched_valid_inputs'] = dict()\n",
    "data['batched_valid_targets'] = dict()\n",
    "\n",
    "batch_size = 50\n",
    "\n",
    "for line in lines:\n",
    "    values = line.split()\n",
    "    sentence = values[-1]\n",
    "    tag = values[0]\n",
    "    cleaned_sentence = cleaner.clean(sentence)\n",
    "    inputs = np.zeros((len(cleaned_sentence), 200))\n",
    "    for word, i in zip(cleaned_sentence, range(len(cleaned_sentence))):\n",
    "        try:\n",
    "            inputs[i] = models.wv[word]\n",
    "        except:\n",
    "            continue\n",
    "    targets = np.zeros(len(target_set))\n",
    "    targets[target_set.index(tag)] = 1.0\n",
    "    key = len(inputs)\n",
    "    if key not in data['train_inputs'].keys():\n",
    "        data['train_inputs'][key] = []\n",
    "        data['train_targets'][key] = []\n",
    "    data['train_inputs'][key].append(inputs)\n",
    "    data['train_targets'][key].append(targets)\n",
    "    \n",
    "data['train_keys'] = data['train_inputs'].keys()\n",
    "print('Feature Extraction for training data finished.')\n",
    "\n",
    "\n",
    "for key in data['train_keys']:\n",
    "    count = int(len(data['train_inputs'][key])/batch_size)+1\n",
    "    for i in range(count):\n",
    "        new_key = str(key)+'_'+str(i)\n",
    "        if i+1 != count:\n",
    "            data['batched_train_inputs'][new_key] = np.array(data['train_inputs'][key][i*batch_size:(i+1)*batch_size])\n",
    "            data['batched_train_targets'][new_key] = np.array(data['train_targets'][key][i*batch_size:(i+1)*batch_size])\n",
    "        else:\n",
    "            data['batched_train_inputs'][new_key] = np.array(data['train_inputs'][key][i*batch_size:])\n",
    "            data['batched_train_targets'][new_key] = np.array(data['train_targets'][key][i*batch_size:])\n",
    "    if count != 1:\n",
    "        if (data['batched_train_inputs'][str(key)+'_1'][0] == data['train_inputs'][key][batch_size+1]).all() == True:\n",
    "            print('SUCCESS!')\n",
    "            \n",
    "print('Batch data extraction finished.')\n",
    "\n",
    "lines = open('../data/TestSet-eCarX-171019.txt', encoding='gbk').readlines()\n",
    "for line in lines:\n",
    "    values = line.split('#')\n",
    "    sentence = values[0]\n",
    "    tag = values[2]\n",
    "    cleaned_sentence = cleaner.clean(sentence)\n",
    "    inputs = np.zeros((len(cleaned_sentence), 200))\n",
    "    for word, i in zip(cleaned_sentence, range(len(cleaned_sentence))):\n",
    "        try:\n",
    "            inputs[i] = models.wv[word]\n",
    "        except:\n",
    "            continue\n",
    "    targets = np.zeros(len(target_set))\n",
    "    targets[target_set.index(tag)] = 1.0\n",
    "    key = len(inputs)\n",
    "    if key not in data['valid_inputs'].keys():\n",
    "        data['valid_inputs'][key] = []\n",
    "        data['valid_targets'][key] = []\n",
    "    data['valid_inputs'][key].append(inputs)\n",
    "    data['valid_targets'][key].append(targets)\n",
    "#     break\n",
    "data['valid_keys'] = data['valid_inputs'].keys()\n",
    "print('Feature Extraction for validation data finished.')\n",
    "\n",
    "for key in data['valid_keys']:\n",
    "    count = int(len(data['valid_inputs'][key])/batch_size)+1\n",
    "    for i in range(count):\n",
    "        new_key = str(key)+'_'+str(i)\n",
    "        if i+1 != count:\n",
    "            data['batched_valid_inputs'][new_key] = np.array(data['valid_inputs'][key][i*batch_size:(i+1)*batch_size])\n",
    "            data['batched_valid_targets'][new_key] = np.array(data['valid_targets'][key][i*batch_size:(i+1)*batch_size])\n",
    "        else:\n",
    "            data['batched_valid_inputs'][new_key] = np.array(data['valid_inputs'][key][i*batch_size:])\n",
    "            data['batched_valid_targets'][new_key] = np.array(data['valid_targets'][key][i*batch_size:])\n",
    "    if count != 1:\n",
    "        if (data['batched_valid_inputs'][str(key)+'_1'][0] == data['valid_inputs'][key][batch_size+1]).all() == True:\n",
    "            print('SUCCESS!')\n",
    "print('Batch data extraction finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(data['train_inputs'], open('../word2vec/gensim_word2vec_train_inputs.npz', 'wb'))\n",
    "pickle.dump(data['train_targets'], open('../word2vec/gensim_word2vec_train_targets.npz', 'wb'))\n",
    "pickle.dump(data['valid_inputs'], open('../word2vec/gensim_word2vec_valid_inputs.npz', 'wb'))\n",
    "pickle.dump(data['valid_targets'], open('../word2vec/gensim_word2vec_valid_targets.npz', 'wb'))\n",
    "pickle.dump(data['label_map'], open('../word2vec/gensim_word2vec_label_map.npz', 'wb'))\n",
    "pickle.dump(data['batched_train_inputs'], open('../word2vec/gensim_word2vec_batched_train_inputs.npz', 'wb'))\n",
    "pickle.dump(data['batched_train_targets'], open('../word2vec/gensim_word2vec_batched_train_targets.npz', 'wb'))\n",
    "pickle.dump(data['batched_valid_inputs'], open('../word2vec/gensim_word2vec_batched_valid_inputs.npz', 'wb'))\n",
    "pickle.dump(data['batched_valid_targets'], open('../word2vec/gensim_word2vec_batched_valid_targets.npz', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Use id to generate word vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cleaner = corpusClean()\n",
    "train_lines = open('../data/TrainSet-eCarX-171019.txt').readlines()\n",
    "vocabulary = set()\n",
    "for line in train_lines:\n",
    "    values = line.split()\n",
    "    tag = values[0]\n",
    "    sentence = values[-1]\n",
    "    cleaned_sentence = cleaner.clean(sentence)\n",
    "    for word in cleaned_sentence:\n",
    "        vocabulary.add(word)\n",
    "vocabulary.add('UNKNOWN')\n",
    "vocabulary = list(vocabulary)\n",
    "pickle.dump(vocabulary, open('../word2vec/dictionary.dict', 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary = pickle.load(open('../word2vec/dictionary.dict', 'rb'))\n",
    "label_map = pickle.load(open('../word2vec/gensim_word2vec_label_map.npz', 'rb'))\n",
    "anouymous = open('../word2vec/train.txt','w')\n",
    "for line in train_lines:\n",
    "    values = line.split()\n",
    "    tag = values[0]\n",
    "    sentence = values[-1]\n",
    "    cleaned_sentence = cleaner.clean(sentence)\n",
    "    ids = [None]*len(cleaned_sentence)\n",
    "    for word, i in zip(cleaned_sentence, range(len(cleaned_sentence))):\n",
    "        if word not in dictionary:\n",
    "            ids[i] = dictionary.index('UNKNOWN')\n",
    "        else:\n",
    "            ids[i] = dictionary.index(word)\n",
    "    save_sentence = [str(label_map.index(tag))] + [str(i) for i in ids]\n",
    "#     print(save_sentence)\n",
    "    save_sentence = ' '.join(save_sentence)\n",
    "#     print(save_sentence)\n",
    "#     print(ids)\n",
    "    anouymous.write(save_sentence+'\\n')\n",
    "anouymous.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "anouymous = open('../word2vec/valid.txt','w')\n",
    "valid_lines = open('../data/TestSet-eCarX-171019.txt').readlines()\n",
    "for line in valid_lines:\n",
    "    values = line.split('#')\n",
    "    tag = values[2]\n",
    "    sentence = values[0]\n",
    "    cleaned_sentence = cleaner.clean(sentence)\n",
    "    ids = [None]*len(cleaned_sentence)\n",
    "    for word, i in zip(cleaned_sentence, range(len(cleaned_sentence))):\n",
    "        if word not in dictionary:\n",
    "            ids[i] = dictionary.index('UNKNOWN')\n",
    "        else:\n",
    "            ids[i] = dictionary.index(word)\n",
    "    save_sentence = [str(label_map.index(tag))] + [str(i) for i in ids]\n",
    "#     print(save_sentence)\n",
    "    save_sentence = ' '.join(save_sentence)\n",
    "#     print(save_sentence)\n",
    "#     print(ids)\n",
    "    anouymous.write(save_sentence+'\\n')\n",
    "anouymous.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lines = open('../word2vec/train.txt').readlines()\n",
    "corpus = [line.split()[1:] for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vector_size = 200\n",
    "ngram = 3\n",
    "min_count = 2\n",
    "workers = 4\n",
    "\n",
    "models = gensim.models.Word2Vec(corpus, size=vector_size, window=ngram, min_count=min_count, workers=workers)\n",
    "models.save('../word2vec/word2vec_anouymous.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class dataProvider():\n",
    "    \n",
    "    def __init__(self,filename, models, label_map):\n",
    "        \n",
    "        self.filename = filename\n",
    "        self.models = models\n",
    "        self.size = self.models.vector_size\n",
    "        self.label_map = label_map\n",
    "        self.data = dict()\n",
    "        self.data['inputs'] = dict()\n",
    "        self.data['targets'] = dict()\n",
    "        self.lines = open(filename).readlines()\n",
    "    \n",
    "    def extract(self):\n",
    "        \n",
    "        for line in self.lines:\n",
    "            values = line.split()\n",
    "            words = values[1:]\n",
    "            tag = values[0]\n",
    "            key = len(words)\n",
    "            inputs = np.zeros((len(words), self.size))\n",
    "            targets = np.zeros(len(self.label_map))\n",
    "            for word, i in zip(words, range(len(words))):\n",
    "                try:\n",
    "                    inputs[i] = self.models.wv[word]\n",
    "                except:\n",
    "                    continue\n",
    "            targets[int(tag)] = 1.0\n",
    "            if key not in self.data['inputs'].keys():\n",
    "                self.data['inputs'][key] = []\n",
    "                self.data['targets'][key] = []\n",
    "            self.data['inputs'][key].append(inputs)\n",
    "            self.data['targets'][key].append(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models = gensim.models.Word2Vec.load('../word2vec/word2vec_anouymous.model')\n",
    "label_map = pickle.load(open('../word2vec/gensim_word2vec_label_map.npz', 'rb'))\n",
    "train_provider = dataProvider('../word2vec/train.txt', models, label_map)\n",
    "train_provider.extract()\n",
    "pickle.dump(train_provider.data, open('../word2vec/anouymous_train_data.npz', 'wb'))\n",
    "\n",
    "valid_provider = dataProvider('../word2vec/valid.txt', models, label_map)\n",
    "valid_provider.extract()\n",
    "pickle.dump(valid_provider.data, open('../word2vec/anouymous_valid_data.npz', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch(data, batch_size):\n",
    "    \n",
    "    keys = data['inputs'].keys()\n",
    "    results = dict()\n",
    "    results['inputs'] = dict()\n",
    "    results['targets'] = dict()\n",
    "    \n",
    "    for key in keys:\n",
    "        count = int(len(data['inputs'][key])/batch_size)+1\n",
    "        for i in range(count):\n",
    "            new_key = str(key)+'_'+str(i)\n",
    "            if i+1 != count:\n",
    "                results['inputs'][new_key] = np.array(data['inputs'][key][i*batch_size:(i+1)*batch_size])\n",
    "                results['targets'][new_key] = np.array(data['targets'][key][i*batch_size:(i+1)*batch_size])\n",
    "            else:\n",
    "                results['inputs'][new_key] = np.array(data['inputs'][key][i*batch_size:])\n",
    "                results['targets'][new_key] = np.array(data['targets'][key][i*batch_size:])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_batch = batch(train_provider.data, 200)\n",
    "valid_batch = batch(valid_provider.data, 200)\n",
    "pickle.dump(train_batch, open('../word2vec/anouymous_train_data_200.npz', 'wb'))\n",
    "pickle.dump(valid_batch, open('../word2vec/anouymous_valid_data_200.npz', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "\n",
    "    \n",
    "class layer(object):\n",
    "    def __init__(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def __str__(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "class affine_layer(layer):\n",
    "    def __init__(self, name, inputs, shape, activation, reuse=False):\n",
    "        with tf.variable_scope('affine', reuse=reuse):\n",
    "            self.weights = tf.get_variable(name = name+'_weights', initializer=tf.truncated_normal(shape, stddev=0.05), dtype=tf.float32)\n",
    "            self.bias = tf.get_variable(name = name+'_bias', initializer=tf.zeros([shape[-1]]))\n",
    "            self.outputs = activation(tf.add(tf.matmul(inputs, self.weights), self.bias))\n",
    "    def __str__(self):\n",
    "        return self.outputs.__str__()\n",
    "    \n",
    "class conv_layer(layer):\n",
    "    def __init__(self, name, inputs, shape, activation, reuse=False):\n",
    "        with tf.variable_scope('conv', reuse=reuse):\n",
    "            self.kernel = tf.get_variable(name = name+'_kernel', initializer=tf.truncated_normal(shape, stddev=0.05), dtype=tf.float32)\n",
    "            self.bias = tf.get_variable(name = name+'_bias', initializer=tf.zeros([shape[-1]]))\n",
    "            self.outputs = activation(tf.add(tf.nn.conv2d(inputs, self.kernel, padding='VALID', strides=[1,1,1,1]), self.bias))\n",
    "    def __str__(self):\n",
    "        return self.outputs.__str__()\n",
    "    \n",
    "class pooling_layer(layer):\n",
    "    def __init__(self, name, inputs, reuse=False):\n",
    "        with tf.variable_scope('pooling', reuse=reuse):\n",
    "            self.outputs = tf.nn.max_pool(name = name+'_maxpooling', value=inputs, ksize=[1,2,2,1], padding='VALID', strides=[1,2,2,1])\n",
    "    def __str__(self):\n",
    "        return self.outputs.__str__()\n",
    "        \n",
    "class reshape_layer(layer):\n",
    "    def __init__(self, name, inputs, shape, reuse=False):\n",
    "        with tf.variable_scope('reshape', reuse=reuse):\n",
    "            self.outputs = tf.reshape(name=name, tensor=inputs, shape=shape)\n",
    "    def __str__(self):\n",
    "        return self.outputs.__str__()\n",
    "        \n",
    "class deconv_layer(layer):\n",
    "    def __init__(self, name, inputs, kernel_shape, output_shape, activation, reuse=False):\n",
    "        with tf.variable_scope('deconv', reuse=reuse):\n",
    "            self.kernel = tf.get_variable(name = name+'_kernel', initializer=tf.truncated_normal(kernel_shape, stddev=0.05), dtype=tf.float32)\n",
    "            self.bias = tf.get_variable(name = name+'_bias', initializer=tf.zeros([kernel_shape[-2]]))\n",
    "            self.outputs = activation(tf.add(tf.nn.conv2d_transpose(inputs, self.kernel, output_shape=output_shape, padding='VALID', strides=[1,2,2,1]), self.bias))\n",
    "    def __str__(self):\n",
    "        return self.outputs.__str__()\n",
    "    \n",
    "class lstm_layer(layer):\n",
    "    def __init__(self, name, inputs, n_units, reuse=False):\n",
    "        with tf.variable_scope('lstm', reuse=reuse):\n",
    "            self.outputs, self.states = tf.nn.dynamic_rnn(\n",
    "                cell = tf.contrib.rnn.BasicLSTMCell(n_units),\n",
    "                inputs = inputs,\n",
    "                dtype = tf.float32)\n",
    "    def __str__(self):\n",
    "        return self.outputs.__str__()\n",
    "\n",
    "    \n",
    "train = pickle.load(open('../word2vec/anouymous_train_data_50.npz', 'rb'))\n",
    "valid = pickle.load(open('../word2vec/anouymous_valid_data_50.npz', 'rb'))\n",
    "batched_inputs = train['inputs']\n",
    "batched_targets = train['targets']\n",
    "v_batched_inputs = valid['inputs']\n",
    "v_batched_targets = valid['targets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-256-04a0f9a1c43f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     30\u001b[0m                 \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0minput_placeholder\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0minputs_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_placeholder\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mtargets_batch\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m             \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m             \u001b[0maccs\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0macc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m             \u001b[0merrs\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\cheng.bi\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    787\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 789\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    790\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\cheng.bi\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    995\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 997\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    998\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\cheng.bi\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1130\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1132\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1133\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mC:\\Users\\cheng.bi\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1137\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1138\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1139\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1140\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\cheng.bi\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1121\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    input_placeholder = tf.placeholder(tf.float32, [None, None, 200])\n",
    "    target_placeholder = tf.placeholder(tf.float32, [None, 134])\n",
    "    \n",
    "    lstm_layer1 = lstm_layer('lstm1', input_placeholder, 200)\n",
    "    affine1 = affine_layer('affine1', lstm_layer1.outputs[:,-1,:], [200, 256], tf.nn.relu)\n",
    "    affine2 = affine_layer('affine2', affine1.outputs, [256, 256], tf.nn.relu)\n",
    "    affine3 = affine_layer('affine3', affine2.outputs, [256, 134], tf.identity)\n",
    "    \n",
    "    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=affine3.outputs, labels=target_placeholder))\n",
    "    optimizer = tf.train.AdamOptimizer(1e-3).minimize(loss)\n",
    "    accuracy = tf.reduce_sum(tf.cast(tf.equal(tf.argmax(affine3.outputs, 1), tf.argmax(target_placeholder, 1)), tf.float32))\n",
    "    \n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for i in range(500):\n",
    "        \n",
    "        accs = 0.0\n",
    "        errs = 0.0\n",
    "        n = 0\n",
    "        for key in batched_inputs.keys():\n",
    "            inputs_batch = batched_inputs[key]\n",
    "            targets_batch = batched_targets[key]\n",
    "#             print(inputs_batch.shape)\n",
    "#             print(targets_batch.shape)\n",
    "            if len(inputs_batch) == 0:\n",
    "                continue\n",
    "            feed_dict = {input_placeholder:inputs_batch, target_placeholder:targets_batch}\n",
    "            _, acc, err = sess.run([optimizer, accuracy, loss], feed_dict=feed_dict)\n",
    "            accs += acc\n",
    "            errs += err\n",
    "            n += len(inputs_batch)\n",
    "        print('#Training Epoch %d, acc: %f, err: %f, n: %d'%(i, accs/n, errs/n, n))\n",
    "        if (i+1)%5 == 0:\n",
    "            for key in v_batched_inputs.keys():\n",
    "                inputs_batch = v_batched_inputs[key]\n",
    "                targets_batch = v_batched_targets[key]\n",
    "#                 print(inputs_batch.shape)\n",
    "#                 print(targets_batch.shape)\n",
    "                if len(v_inputs_batch) == 0:\n",
    "                    continue\n",
    "                feed_dict = {input_placeholder:inputs_batch, target_placeholder:targets_batch}\n",
    "                acc, err = sess.run([accuracy, loss], feed_dict=feed_dict)\n",
    "                accs += acc\n",
    "                errs += err\n",
    "                n += len(inputs_batch)\n",
    "            print('__________________________________________________________________________')\n",
    "            print('#Testing Epoch %d, acc: %f, err: %f, n: %d'%(i, accs/n, errs/n, n))\n",
    "            print('__________________________________________________________________________')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Generate with padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models = gensim.models.Word2Vec.load('../word2vec/word2vec_anouymous.model')\n",
    "label_map = pickle.load(open('../word2vec/gensim_word2vec_label_map.npz', 'rb'))\n",
    "train_provider = dataProvider('../word2vec/train.txt', models, label_map)\n",
    "train_provider.extract()\n",
    "pickle.dump(train_p rovider.data, open('../word2vec/anouymous_train_data.npz', 'wb'))\n",
    "\n",
    "valid_provider = dataProvider('../word2vec/valid.txt', models, label_map)\n",
    "valid_provider.extract()\n",
    "pickle.dump(valid_provider.data, open('../word2vec/anouymous_valid_data.npz', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
