{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cheng.bi\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:862: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import re\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class corpusClean():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.num_re = re.compile('[0-9一二三四五六七八九十]+')\n",
    "        self.char_re = re.compile('[a-zA-Z\\-\\,\\.\\。\\，]+')\n",
    "\n",
    "    def clean(self, line):\n",
    "        raw_words = [i for i in line]\n",
    "#         print(raw_words)\n",
    "        new_words = []\n",
    "        for i in range(len(raw_words)):\n",
    "            word = raw_words[i]\n",
    "            if 65281 <= ord(word) and ord(word) <= 65375:\n",
    "                word = chr(ord(word)-65248)\n",
    "            if (33 <= ord(word) and ord(word) < 48) or (8208 <= ord(word) and ord(word) <= 8232):\n",
    "                word = ''\n",
    "            new_words.append(word)\n",
    "        sentence = ''.join(new_words)\n",
    "        \n",
    "        removed_char = re.sub(self.char_re, ' <CHAR> ', sentence)\n",
    "        removed_num = re.sub(self.num_re, ' <NUMBER> ', removed_char)\n",
    "        words = removed_num.split()\n",
    "        saved_words = ['<BEGIN>']\n",
    "        for word in words:\n",
    "            if len(word) == 1 or word.find('<') != -1:\n",
    "                saved_words.append(word)\n",
    "            else:\n",
    "                for w in word:\n",
    "                    saved_words.append(w)\n",
    "        saved_words.append('<END>')\n",
    "        return saved_words\n",
    "    \n",
    "\n",
    "    \n",
    "class layer(object):\n",
    "    def __init__(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def __str__(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "class affine_layer(layer):\n",
    "    def __init__(self, name, inputs, shape, activation, reuse=False):\n",
    "        with tf.variable_scope('affine', reuse=reuse):\n",
    "            self.weights = tf.get_variable(name = name+'_weights', initializer=tf.truncated_normal(shape, stddev=0.05), dtype=tf.float32)\n",
    "            self.bias = tf.get_variable(name = name+'_bias', initializer=tf.zeros([shape[-1]]))\n",
    "            self.outputs = activation(tf.add(tf.matmul(inputs, self.weights), self.bias))\n",
    "    def __str__(self):\n",
    "        return self.outputs.__str__()\n",
    "    \n",
    "class conv_layer(layer):\n",
    "    def __init__(self, name, inputs, shape, activation, reuse=False):\n",
    "        with tf.variable_scope('conv', reuse=reuse):\n",
    "            self.kernel = tf.get_variable(name = name+'_kernel', initializer=tf.truncated_normal(shape, stddev=0.05), dtype=tf.float32)\n",
    "            self.bias = tf.get_variable(name = name+'_bias', initializer=tf.zeros([shape[-1]]))\n",
    "            self.outputs = activation(tf.add(tf.nn.conv2d(inputs, self.kernel, padding='VALID', strides=[1,1,1,1]), self.bias))\n",
    "    def __str__(self):\n",
    "        return self.outputs.__str__()\n",
    "    \n",
    "class pooling_layer(layer):\n",
    "    def __init__(self, name, inputs, reuse=False):\n",
    "        with tf.variable_scope('pooling', reuse=reuse):\n",
    "            self.outputs = tf.nn.max_pool(name = name+'_maxpooling', value=inputs, ksize=[1,2,2,1], padding='VALID', strides=[1,2,2,1])\n",
    "    def __str__(self):\n",
    "        return self.outputs.__str__()\n",
    "        \n",
    "class reshape_layer(layer):\n",
    "    def __init__(self, name, inputs, shape, reuse=False):\n",
    "        with tf.variable_scope('reshape', reuse=reuse):\n",
    "            self.outputs = tf.reshape(name=name, tensor=inputs, shape=shape)\n",
    "    def __str__(self):\n",
    "        return self.outputs.__str__()\n",
    "        \n",
    "class deconv_layer(layer):\n",
    "    def __init__(self, name, inputs, kernel_shape, output_shape, activation, reuse=False):\n",
    "        with tf.variable_scope('deconv', reuse=reuse):\n",
    "            self.kernel = tf.get_variable(name = name+'_kernel', initializer=tf.truncated_normal(kernel_shape, stddev=0.05), dtype=tf.float32)\n",
    "            self.bias = tf.get_variable(name = name+'_bias', initializer=tf.zeros([kernel_shape[-2]]))\n",
    "            self.outputs = activation(tf.add(tf.nn.conv2d_transpose(inputs, self.kernel, output_shape=output_shape, padding='VALID', strides=[1,2,2,1]), self.bias))\n",
    "    def __str__(self):\n",
    "        return self.outputs.__str__()\n",
    "    \n",
    "class lstm_layer(layer):\n",
    "    def __init__(self, name, inputs, n_units, reuse=False):\n",
    "        with tf.variable_scope('lstm', reuse=reuse):\n",
    "            self.outputs, self.states = tf.nn.dynamic_rnn(\n",
    "                cell = tf.contrib.rnn.BasicLSTMCell(n_units),\n",
    "                inputs = inputs,\n",
    "                dtype = tf.float32)\n",
    "    def __str__(self):\n",
    "        return self.outputs.__str__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = open('../data/cleaned_corpus.txt', encoding='utf-8').readlines()\n",
    "lines = open('../data/TrainSet-eCarX-171019.txt', encoding='gbk').readlines()\n",
    "vector_size = 200\n",
    "ngram = 3\n",
    "min_count = 2\n",
    "workers = 4\n",
    "# models = gensim.models.Word2Vec.load('../word2vec/word2vec.model')\n",
    "models = gensim.models.Word2Vec(sentences, size=vector_size, window=ngram, min_count=min_count, workers=workers)\n",
    "cleaner = corpusClean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models.save('../word2vec/word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(train_data, open('../word2vec/train_inputs.npz', 'wb'))\n",
    "pickle.dump(train_target_one_hot, open('../word2vec/train_targets.npz', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = pickle.load(open('../word2vec/train_inputs.npz', 'rb'))\n",
    "train_target_one_hot = pickle.load(open('../word2vec/train_targets.npz', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batched_inputs = dict()\n",
    "batched_targets = dict()\n",
    "for key in train_data.keys():\n",
    "    count = int(len(train_data[key])/50)+1\n",
    "    for i in range(count):\n",
    "        new_key = str(key)+'_'+str(i)\n",
    "        if i+1 != count:\n",
    "            batched_inputs[new_key] = train_data[key][i*50:(i+1)*50]\n",
    "            batched_targets[new_key] = train_target_one_hot[key][i*50:(i+1)*50]\n",
    "        else:\n",
    "            batched_inputs[new_key] = train_data[key][i*50:]\n",
    "            batched_targets[new_key] = train_target_one_hot[key][i*50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pickle.dump(batched_inputs, open('../word2vec/batched_inputs.npz', 'wb'))\n",
    "# pickle.dump(batched_targets, open('../word2vec/batched_targets.npz', 'wb'))\n",
    "batched_inputs = pickle.load(open('../word2vec/batched_inputs.npz', 'rb'))\n",
    "batched_targets = pickle.load(open('../word2vec/batched_targets.npz', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Training Epoch 0, acc: 0.004216, err: nan, n: 134000\n",
      "#Training Epoch 1, acc: 0.013530, err: nan, n: 134000\n",
      "#Training Epoch 2, acc: 0.030985, err: nan, n: 134000\n",
      "#Training Epoch 3, acc: 0.063888, err: nan, n: 134000\n",
      "#Training Epoch 4, acc: 0.090015, err: nan, n: 134000\n",
      "#Training Epoch 5, acc: 0.132321, err: nan, n: 134000\n",
      "#Training Epoch 6, acc: 0.174157, err: nan, n: 134000\n",
      "#Training Epoch 7, acc: 0.207254, err: nan, n: 134000\n",
      "#Training Epoch 8, acc: 0.245701, err: nan, n: 134000\n",
      "#Training Epoch 9, acc: 0.284731, err: nan, n: 134000\n",
      "#Training Epoch 10, acc: 0.327642, err: nan, n: 134000\n",
      "#Training Epoch 11, acc: 0.350366, err: nan, n: 134000\n",
      "#Training Epoch 12, acc: 0.376813, err: nan, n: 134000\n",
      "#Training Epoch 13, acc: 0.409552, err: nan, n: 134000\n",
      "#Training Epoch 14, acc: 0.433149, err: nan, n: 134000\n",
      "#Training Epoch 15, acc: 0.458679, err: nan, n: 134000\n",
      "#Training Epoch 16, acc: 0.474097, err: nan, n: 134000\n",
      "#Training Epoch 17, acc: 0.501836, err: nan, n: 134000\n"
     ]
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    input_placeholder = tf.placeholder(tf.float32, [None, None, 200])\n",
    "    target_placeholder = tf.placeholder(tf.float32, [None, 134])\n",
    "    \n",
    "    lstm_layer1 = lstm_layer('lstm1', input_placeholder, 200)\n",
    "    affine1 = affine_layer('affine1', lstm_layer1.outputs[:,-1,:], [200, 256], tf.nn.relu)\n",
    "    affine2 = affine_layer('affine2', affine1.outputs, [256, 256], tf.nn.relu)\n",
    "    affine3 = affine_layer('affine3', affine2.outputs, [256, 134], tf.identity)\n",
    "    \n",
    "    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=affine3.outputs, labels=target_placeholder))\n",
    "    optimizer = tf.train.AdamOptimizer(1e-3).minimize(loss)\n",
    "    accuracy = tf.reduce_sum(tf.cast(tf.equal(tf.argmax(affine3.outputs, 1), tf.argmax(target_placeholder, 1)), tf.float32))\n",
    "    \n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for i in range(500):\n",
    "        \n",
    "        accs = 0.0\n",
    "        errs = 0.0\n",
    "        n = 0\n",
    "        for key in batched_inputs.keys():\n",
    "            inputs_batch = batched_inputs[key]\n",
    "            targets_batch = batched_targets[key]\n",
    "#             print(inputs_batch.shape)\n",
    "#             print(targets_batch.shape)\n",
    "            feed_dict = {input_placeholder:inputs_batch, target_placeholder:targets_batch}\n",
    "            _, acc, err = sess.run([optimizer, accuracy, loss], feed_dict=feed_dict)\n",
    "            accs += acc\n",
    "            errs += err\n",
    "            n += len(inputs_batch)\n",
    "        print('#Training Epoch %d, acc: %f, err: %f, n: %d'%(i, accs/n, errs/n, n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use raw corpus to generate word vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_size = 200\n",
    "ngram = 3\n",
    "min_count = 2\n",
    "workers = 4\n",
    "lines = open('../data/cleaned_corpus.txt', encoding='utf-8').readlines()\n",
    "sentences = [i.split() for i in lines]\n",
    "# print(sentences[0])\n",
    "models = gensim.models.Word2Vec(sentences, size=vector_size, window=ngram, min_count=min_count, workers=workers)\n",
    "models.save('../word2vec/word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134\n",
      "Feature Extraction for training data finished.\n",
      "SUCCESS!\n",
      "Batch data extraction finished.\n",
      "Feature Extraction for validation data finished.\n",
      "SUCCESS!\n",
      "SUCCESS!\n",
      "SUCCESS!\n",
      "Batch data extraction finished.\n"
     ]
    }
   ],
   "source": [
    "lines = open('../data/TrainSet-eCarX-171019.txt').readlines()\n",
    "target_set = set()\n",
    "for line in lines:\n",
    "    target = line.split()[0]\n",
    "    target_set.add(target)\n",
    "target_set = list(target_set)\n",
    "print(len(target_set))\n",
    "models = gensim.models.Word2Vec.load('../word2vec/word2vec.model')\n",
    "cleaner = corpusClean()\n",
    "data = dict()\n",
    "data['train_inputs'] = dict()\n",
    "data['train_targets'] = dict()\n",
    "data['valid_inputs'] = dict()\n",
    "data['valid_targets'] = dict()\n",
    "data['label_map'] = target_set\n",
    "data['batched_train_inputs'] = dict()\n",
    "data['batched_train_targets'] = dict()\n",
    "data['batched_valid_inputs'] = dict()\n",
    "data['batched_valid_targets'] = dict()\n",
    "\n",
    "batch_size = 50\n",
    "\n",
    "for line in lines:\n",
    "    values = line.split()\n",
    "    sentence = values[-1]\n",
    "    tag = values[0]\n",
    "    cleaned_sentence = cleaner.clean(sentence)\n",
    "    inputs = np.zeros((len(cleaned_sentence), 200))\n",
    "    for word, i in zip(cleaned_sentence, range(len(cleaned_sentence))):\n",
    "        try:\n",
    "            inputs[i] = models.wv[word]\n",
    "        except:\n",
    "            continue\n",
    "    targets = np.zeros(len(target_set))\n",
    "    targets[target_set.index(tag)] = 1.0\n",
    "    key = len(inputs)\n",
    "    if key not in data['train_inputs'].keys():\n",
    "        data['train_inputs'][key] = []\n",
    "        data['train_targets'][key] = []\n",
    "    data['train_inputs'][key].append(inputs)\n",
    "    data['train_targets'][key].append(targets)\n",
    "    \n",
    "data['train_keys'] = data['train_inputs'].keys()\n",
    "print('Feature Extraction for training data finished.')\n",
    "\n",
    "\n",
    "for key in data['train_keys']:\n",
    "    count = int(len(data['train_inputs'][key])/batch_size)+1\n",
    "    for i in range(count):\n",
    "        new_key = str(key)+'_'+str(i)\n",
    "        if i+1 != count:\n",
    "            data['batched_train_inputs'][new_key] = np.array(data['train_inputs'][key][i*batch_size:(i+1)*batch_size])\n",
    "            data['batched_train_targets'][new_key] = np.array(data['train_targets'][key][i*batch_size:(i+1)*batch_size])\n",
    "        else:\n",
    "            data['batched_train_inputs'][new_key] = np.array(data['train_inputs'][key][i*batch_size:])\n",
    "            data['batched_train_targets'][new_key] = np.array(data['train_targets'][key][i*batch_size:])\n",
    "    if count != 1:\n",
    "        if (data['batched_train_inputs'][str(key)+'_1'][0] == data['train_inputs'][key][batch_size+1]).all() == True:\n",
    "            print('SUCCESS!')\n",
    "            \n",
    "print('Batch data extraction finished.')\n",
    "\n",
    "lines = open('../data/TestSet-eCarX-171019.txt', encoding='gbk').readlines()\n",
    "for line in lines:\n",
    "    values = line.split('#')\n",
    "    sentence = values[0]\n",
    "    tag = values[2]\n",
    "    cleaned_sentence = cleaner.clean(sentence)\n",
    "    inputs = np.zeros((len(cleaned_sentence), 200))\n",
    "    for word, i in zip(cleaned_sentence, range(len(cleaned_sentence))):\n",
    "        try:\n",
    "            inputs[i] = models.wv[word]\n",
    "        except:\n",
    "            continue\n",
    "    targets = np.zeros(len(target_set))\n",
    "    targets[target_set.index(tag)] = 1.0\n",
    "    key = len(inputs)\n",
    "    if key not in data['valid_inputs'].keys():\n",
    "        data['valid_inputs'][key] = []\n",
    "        data['valid_targets'][key] = []\n",
    "    data['valid_inputs'][key].append(inputs)\n",
    "    data['valid_targets'][key].append(targets)\n",
    "#     break\n",
    "data['valid_keys'] = data['valid_inputs'].keys()\n",
    "print('Feature Extraction for validation data finished.')\n",
    "\n",
    "for key in data['valid_keys']:\n",
    "    count = int(len(data['valid_inputs'][key])/batch_size)+1\n",
    "    for i in range(count):\n",
    "        new_key = str(key)+'_'+str(i)\n",
    "        if i+1 != count:\n",
    "            data['batched_valid_inputs'][new_key] = np.array(data['valid_inputs'][key][i*batch_size:(i+1)*batch_size])\n",
    "            data['batched_valid_targets'][new_key] = np.array(data['valid_targets'][key][i*batch_size:(i+1)*batch_size])\n",
    "        else:\n",
    "            data['batched_valid_inputs'][new_key] = np.array(data['valid_inputs'][key][i*batch_size:])\n",
    "            data['batched_valid_targets'][new_key] = np.array(data['valid_targets'][key][i*batch_size:])\n",
    "    if count != 1:\n",
    "        if (data['batched_valid_inputs'][str(key)+'_1'][0] == data['valid_inputs'][key][batch_size+1]).all() == True:\n",
    "            print('SUCCESS!')\n",
    "print('Batch data extraction finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(data['train_inputs'], open('../word2vec/gensim_word2vec_train_inputs.npz', 'wb'))\n",
    "pickle.dump(data['train_targets'], open('../word2vec/gensim_word2vec_train_targets.npz', 'wb'))\n",
    "pickle.dump(data['valid_inputs'], open('../word2vec/gensim_word2vec_valid_inputs.npz', 'wb'))\n",
    "pickle.dump(data['valid_targets'], open('../word2vec/gensim_word2vec_valid_targets.npz', 'wb'))\n",
    "pickle.dump(data['label_map'], open('../word2vec/gensim_word2vec_label_map.npz', 'wb'))\n",
    "pickle.dump(data['batched_train_inputs'], open('../word2vec/gensim_word2vec_batched_train_inputs.npz', 'wb'))\n",
    "pickle.dump(data['batched_train_targets'], open('../word2vec/gensim_word2vec_batched_train_targets.npz', 'wb'))\n",
    "pickle.dump(data['batched_valid_inputs'], open('../word2vec/gensim_word2vec_batched_valid_inputs.npz', 'wb'))\n",
    "pickle.dump(data['batched_valid_targets'], open('../word2vec/gensim_word2vec_batched_valid_targets.npz', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Use id to generate word vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaner = corpusClean()\n",
    "train_lines = open('../data/TrainSet-eCarX-171019.txt').readlines()\n",
    "vocabulary = set()\n",
    "for line in train_lines:\n",
    "    values = line.split()\n",
    "    tag = values[0]\n",
    "    sentence = values[-1]\n",
    "    cleaned_sentence = cleaner.clean(sentence)\n",
    "    for word in cleaned_sentence:\n",
    "        vocabulary.add(word)\n",
    "vocabulary.add('UNKNOWN')\n",
    "vocabulary = list(vocabulary)\n",
    "pickle.dump(vocabulary, open('../word2vec/dictionary.dict', 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = pickle.load(open('../word2vec/dictionary.dict', 'rb'))\n",
    "label_map = pickle.load(open('../word2vec/gensim_word2vec_label_map.npz', 'rb'))\n",
    "anouymous = open('../word2vec/train.txt','w')\n",
    "for line in train_lines:\n",
    "    values = line.split()\n",
    "    tag = values[0]\n",
    "    sentence = values[-1]\n",
    "    cleaned_sentence = cleaner.clean(sentence)\n",
    "    ids = [None]*len(cleaned_sentence)\n",
    "    for word, i in zip(cleaned_sentence, range(len(cleaned_sentence))):\n",
    "        if word not in dictionary:\n",
    "            ids[i] = dictionary.index('UNKNOWN')\n",
    "        else:\n",
    "            ids[i] = dictionary.index(word)\n",
    "    save_sentence = [str(label_map.index(tag))] + [str(i) for i in ids]\n",
    "#     print(save_sentence)\n",
    "    save_sentence = ' '.join(save_sentence)\n",
    "#     print(save_sentence)\n",
    "#     print(ids)\n",
    "    anouymous.write(save_sentence+'\\n')\n",
    "anouymous.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "anouymous = open('../word2vec/valid.txt','w')\n",
    "valid_lines = open('../data/TestSet-eCarX-171019.txt').readlines()\n",
    "for line in valid_lines:\n",
    "    values = line.split('#')\n",
    "    tag = values[2]\n",
    "    sentence = values[0]\n",
    "    cleaned_sentence = cleaner.clean(sentence)\n",
    "    ids = [None]*len(cleaned_sentence)\n",
    "    for word, i in zip(cleaned_sentence, range(len(cleaned_sentence))):\n",
    "        if word not in dictionary:\n",
    "            ids[i] = dictionary.index('UNKNOWN')\n",
    "        else:\n",
    "            ids[i] = dictionary.index(word)\n",
    "    save_sentence = [str(label_map.index(tag))] + [str(i) for i in ids]\n",
    "#     print(save_sentence)\n",
    "    save_sentence = ' '.join(save_sentence)\n",
    "#     print(save_sentence)\n",
    "#     print(ids)\n",
    "    anouymous.write(save_sentence+'\\n')\n",
    "anouymous.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = open('../word2vec/train.txt').readlines()\n",
    "corpus = [line.split()[1:] for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_size = 200\n",
    "ngram = 3\n",
    "min_count = 2\n",
    "workers = 4\n",
    "\n",
    "models = gensim.models.Word2Vec(corpus, size=vector_size, window=ngram, min_count=min_count, workers=workers)\n",
    "models.save('../word2vec/word2vec_anouymous.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataProvider():\n",
    "    \n",
    "    def __init__(self,filename, models, label_map):\n",
    "        \n",
    "        self.filename = filename\n",
    "        self.models = models\n",
    "        self.size = self.models.vector_size\n",
    "        self.label_map = label_map\n",
    "        self.data = dict()\n",
    "        self.data['inputs'] = dict()\n",
    "        self.data['targets'] = dict()\n",
    "        self.lines = open(filename).readlines()\n",
    "    \n",
    "    def extract(self):\n",
    "        \n",
    "        for line in self.lines:\n",
    "            values = line.split()\n",
    "            words = values[1:]\n",
    "            tag = values[0]\n",
    "            key = len(words)\n",
    "            inputs = np.zeros((len(words), self.size))\n",
    "            targets = np.zeros(len(self.label_map))\n",
    "            for word, i in zip(words, range(len(words))):\n",
    "                try:\n",
    "                    inputs[i] = self.models.wv[word]\n",
    "                except:\n",
    "                    continue\n",
    "            targets[int(tag)] = 1.0\n",
    "            if key not in self.data['inputs'].keys():\n",
    "                self.data['inputs'][key] = []\n",
    "                self.data['targets'][key] = []\n",
    "            self.data['inputs'][key].append(inputs)\n",
    "            self.data['targets'][key].append(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = gensim.models.Word2Vec.load('../word2vec/word2vec_anouymous.model')\n",
    "\n",
    "train_provider = dataProvider('../word2vec/train.txt', models, label_map)\n",
    "train_provider.extract()\n",
    "pickle.dump(train_provider.data, open('../word2vec/anouymous_train_data.npz', 'wb'))\n",
    "\n",
    "valid_provider = dataProvider('../word2vec/valid.txt', models, label_map)\n",
    "valid_provider.extract()\n",
    "pickle.dump(valid_provider.data, open('../word2vec/anouymous_valid_data.npz', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch(data, batch_size):\n",
    "    \n",
    "    keys = data['inputs'].keys()\n",
    "    results = dict()\n",
    "    results['inputs'] = dict()\n",
    "    results['targets'] = dict()\n",
    "    \n",
    "    for key in keys:\n",
    "        count = int(len(data['inputs'][key])/batch_size)+1\n",
    "        for i in range(count):\n",
    "            new_key = str(key)+'_'+str(i)\n",
    "            if i+1 != count:\n",
    "                results['inputs'][new_key] = np.array(data['inputs'][key][i*batch_size:(i+1)*batch_size])\n",
    "                results['targets'][new_key] = np.array(data['targets'][key][i*batch_size:(i+1)*batch_size])\n",
    "            else:\n",
    "                results['inputs'][new_key] = np.array(data['inputs'][key][i*batch_size:])\n",
    "                results['targets'][new_key] = np.array(data['targets'][key][i*batch_size:])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch = batch(train_provider.data, 50)\n",
    "valid_batch = batch(valid_provider.data, 50)\n",
    "pickle.dump(train_batch, open('../word2vec/anouymous_train_data_50.npz'))\n",
    "pickle.dump(valid_batch, open('../word2vec/anouymous_valid_data_50.npz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['13_0', '13_1', '13_2', '13_3', '13_4', '13_5', '13_6', '13_7', '13_8', '13_9', '13_10', '13_11', '13_12', '13_13', '13_14', '13_15', '13_16', '13_17', '13_18', '13_19', '13_20', '13_21', '13_22', '13_23', '13_24', '13_25', '13_26', '13_27', '13_28', '13_29', '13_30', '13_31', '13_32', '13_33', '13_34', '13_35', '13_36', '13_37', '13_38', '13_39', '13_40', '13_41', '13_42', '13_43', '13_44', '13_45', '13_46', '13_47', '13_48', '13_49', '13_50', '13_51', '13_52', '13_53', '13_54', '13_55', '13_56', '13_57', '13_58', '13_59', '13_60', '13_61', '13_62', '13_63', '13_64', '13_65', '13_66', '13_67', '13_68', '13_69', '13_70', '13_71', '13_72', '13_73', '13_74', '13_75', '13_76', '13_77', '13_78', '13_79', '13_80', '13_81', '13_82', '13_83', '13_84', '13_85', '13_86', '13_87', '13_88', '13_89', '13_90', '13_91', '13_92', '13_93', '13_94', '13_95', '13_96', '13_97', '13_98', '13_99', '13_100', '13_101', '13_102', '13_103', '13_104', '13_105', '13_106', '13_107', '13_108', '13_109', '13_110', '13_111', '13_112', '13_113', '13_114', '13_115', '13_116', '13_117', '11_0', '11_1', '11_2', '11_3', '11_4', '11_5', '11_6', '11_7', '11_8', '11_9', '11_10', '11_11', '11_12', '11_13', '11_14', '11_15', '11_16', '11_17', '11_18', '11_19', '11_20', '11_21', '11_22', '11_23', '11_24', '11_25', '11_26', '11_27', '11_28', '11_29', '11_30', '11_31', '11_32', '11_33', '11_34', '11_35', '11_36', '11_37', '11_38', '11_39', '11_40', '11_41', '11_42', '11_43', '11_44', '11_45', '11_46', '11_47', '11_48', '11_49', '11_50', '11_51', '11_52', '11_53', '11_54', '11_55', '11_56', '11_57', '11_58', '11_59', '11_60', '11_61', '11_62', '11_63', '11_64', '11_65', '11_66', '11_67', '11_68', '11_69', '11_70', '11_71', '11_72', '11_73', '11_74', '11_75', '11_76', '11_77', '11_78', '11_79', '11_80', '11_81', '11_82', '11_83', '11_84', '11_85', '11_86', '11_87', '11_88', '11_89', '11_90', '11_91', '11_92', '11_93', '11_94', '11_95', '11_96', '11_97', '11_98', '11_99', '11_100', '11_101', '11_102', '11_103', '11_104', '11_105', '11_106', '11_107', '11_108', '11_109', '11_110', '11_111', '11_112', '11_113', '11_114', '11_115', '11_116', '11_117', '11_118', '11_119', '11_120', '11_121', '11_122', '11_123', '11_124', '11_125', '11_126', '11_127', '11_128', '11_129', '11_130', '11_131', '11_132', '11_133', '11_134', '11_135', '11_136', '11_137', '11_138', '11_139', '11_140', '11_141', '11_142', '11_143', '11_144', '11_145', '11_146', '11_147', '11_148', '11_149', '11_150', '11_151', '11_152', '11_153', '11_154', '11_155', '11_156', '11_157', '11_158', '11_159', '11_160', '11_161', '11_162', '11_163', '11_164', '11_165', '11_166', '11_167', '11_168', '11_169', '11_170', '11_171', '11_172', '11_173', '11_174', '11_175', '11_176', '11_177', '11_178', '11_179', '11_180', '11_181', '11_182', '11_183', '11_184', '11_185', '11_186', '11_187', '11_188', '11_189', '11_190', '11_191', '11_192', '11_193', '11_194', '11_195', '11_196', '11_197', '11_198', '11_199', '11_200', '11_201', '11_202', '11_203', '11_204', '11_205', '11_206', '11_207', '11_208', '11_209', '11_210', '11_211', '11_212', '11_213', '19_0', '19_1', '19_2', '19_3', '19_4', '19_5', '19_6', '19_7', '19_8', '19_9', '19_10', '15_0', '15_1', '15_2', '15_3', '15_4', '15_5', '15_6', '15_7', '15_8', '15_9', '15_10', '15_11', '15_12', '15_13', '15_14', '15_15', '15_16', '15_17', '15_18', '15_19', '15_20', '15_21', '15_22', '15_23', '15_24', '15_25', '15_26', '15_27', '15_28', '15_29', '15_30', '15_31', '15_32', '15_33', '15_34', '15_35', '15_36', '15_37', '15_38', '15_39', '15_40', '15_41', '15_42', '15_43', '15_44', '15_45', '15_46', '15_47', '15_48', '12_0', '12_1', '12_2', '12_3', '12_4', '12_5', '12_6', '12_7', '12_8', '12_9', '12_10', '12_11', '12_12', '12_13', '12_14', '12_15', '12_16', '12_17', '12_18', '12_19', '12_20', '12_21', '12_22', '12_23', '12_24', '12_25', '12_26', '12_27', '12_28', '12_29', '12_30', '12_31', '12_32', '12_33', '12_34', '12_35', '12_36', '12_37', '12_38', '12_39', '12_40', '12_41', '12_42', '12_43', '12_44', '12_45', '12_46', '12_47', '12_48', '12_49', '12_50', '12_51', '12_52', '12_53', '12_54', '12_55', '12_56', '12_57', '12_58', '12_59', '12_60', '12_61', '12_62', '12_63', '12_64', '12_65', '12_66', '12_67', '12_68', '12_69', '12_70', '12_71', '12_72', '12_73', '12_74', '12_75', '12_76', '12_77', '12_78', '12_79', '12_80', '12_81', '12_82', '12_83', '12_84', '12_85', '12_86', '12_87', '12_88', '12_89', '12_90', '12_91', '12_92', '12_93', '12_94', '12_95', '12_96', '12_97', '12_98', '12_99', '12_100', '12_101', '12_102', '12_103', '12_104', '12_105', '12_106', '12_107', '12_108', '12_109', '12_110', '12_111', '12_112', '12_113', '12_114', '12_115', '12_116', '12_117', '12_118', '12_119', '12_120', '12_121', '12_122', '12_123', '12_124', '12_125', '12_126', '12_127', '12_128', '12_129', '12_130', '12_131', '12_132', '12_133', '12_134', '12_135', '12_136', '12_137', '12_138', '12_139', '12_140', '12_141', '12_142', '12_143', '12_144', '12_145', '12_146', '12_147', '12_148', '12_149', '12_150', '12_151', '12_152', '12_153', '12_154', '12_155', '12_156', '10_0', '10_1', '10_2', '10_3', '10_4', '10_5', '10_6', '10_7', '10_8', '10_9', '10_10', '10_11', '10_12', '10_13', '10_14', '10_15', '10_16', '10_17', '10_18', '10_19', '10_20', '10_21', '10_22', '10_23', '10_24', '10_25', '10_26', '10_27', '10_28', '10_29', '10_30', '10_31', '10_32', '10_33', '10_34', '10_35', '10_36', '10_37', '10_38', '10_39', '10_40', '10_41', '10_42', '10_43', '10_44', '10_45', '10_46', '10_47', '10_48', '10_49', '10_50', '10_51', '10_52', '10_53', '10_54', '10_55', '10_56', '10_57', '10_58', '10_59', '10_60', '10_61', '10_62', '10_63', '10_64', '10_65', '10_66', '10_67', '10_68', '10_69', '10_70', '10_71', '10_72', '10_73', '10_74', '10_75', '10_76', '10_77', '10_78', '10_79', '10_80', '10_81', '10_82', '10_83', '10_84', '10_85', '10_86', '10_87', '10_88', '10_89', '10_90', '10_91', '10_92', '10_93', '10_94', '10_95', '10_96', '10_97', '10_98', '10_99', '10_100', '10_101', '10_102', '10_103', '10_104', '10_105', '10_106', '10_107', '10_108', '10_109', '10_110', '10_111', '10_112', '10_113', '10_114', '10_115', '10_116', '10_117', '10_118', '10_119', '10_120', '10_121', '10_122', '10_123', '10_124', '10_125', '10_126', '10_127', '10_128', '10_129', '10_130', '10_131', '10_132', '10_133', '10_134', '10_135', '10_136', '10_137', '10_138', '10_139', '10_140', '10_141', '10_142', '10_143', '10_144', '10_145', '10_146', '10_147', '10_148', '10_149', '10_150', '10_151', '10_152', '10_153', '10_154', '10_155', '10_156', '10_157', '10_158', '10_159', '10_160', '10_161', '10_162', '10_163', '10_164', '10_165', '10_166', '10_167', '10_168', '10_169', '10_170', '10_171', '10_172', '10_173', '10_174', '10_175', '10_176', '10_177', '10_178', '10_179', '10_180', '10_181', '10_182', '10_183', '10_184', '10_185', '10_186', '10_187', '10_188', '10_189', '10_190', '10_191', '10_192', '10_193', '10_194', '10_195', '10_196', '10_197', '10_198', '10_199', '10_200', '10_201', '10_202', '10_203', '10_204', '10_205', '10_206', '10_207', '10_208', '10_209', '10_210', '10_211', '10_212', '10_213', '10_214', '10_215', '10_216', '10_217', '10_218', '10_219', '10_220', '10_221', '10_222', '10_223', '10_224', '10_225', '10_226', '10_227', '10_228', '10_229', '10_230', '10_231', '10_232', '10_233', '10_234', '10_235', '10_236', '10_237', '10_238', '10_239', '10_240', '10_241', '10_242', '10_243', '10_244', '10_245', '10_246', '10_247', '10_248', '10_249', '10_250', '10_251', '10_252', '10_253', '10_254', '10_255', '10_256', '10_257', '10_258', '10_259', '10_260', '10_261', '10_262', '10_263', '10_264', '10_265', '10_266', '10_267', '10_268', '10_269', '10_270', '10_271', '10_272', '10_273', '10_274', '10_275', '10_276', '10_277', '10_278', '10_279', '10_280', '10_281', '10_282', '10_283', '10_284', '10_285', '10_286', '10_287', '10_288', '10_289', '10_290', '10_291', '10_292', '10_293', '10_294', '10_295', '10_296', '10_297', '10_298', '10_299', '10_300', '17_0', '17_1', '17_2', '17_3', '17_4', '17_5', '17_6', '17_7', '17_8', '17_9', '17_10', '17_11', '17_12', '17_13', '17_14', '17_15', '17_16', '9_0', '9_1', '9_2', '9_3', '9_4', '9_5', '9_6', '9_7', '9_8', '9_9', '9_10', '9_11', '9_12', '9_13', '9_14', '9_15', '9_16', '9_17', '9_18', '9_19', '9_20', '9_21', '9_22', '9_23', '9_24', '9_25', '9_26', '9_27', '9_28', '9_29', '9_30', '9_31', '9_32', '9_33', '9_34', '9_35', '9_36', '9_37', '9_38', '9_39', '9_40', '9_41', '9_42', '9_43', '9_44', '9_45', '9_46', '9_47', '9_48', '9_49', '9_50', '9_51', '9_52', '9_53', '9_54', '9_55', '9_56', '9_57', '9_58', '9_59', '9_60', '9_61', '9_62', '9_63', '9_64', '9_65', '9_66', '9_67', '9_68', '9_69', '9_70', '9_71', '9_72', '9_73', '9_74', '9_75', '9_76', '9_77', '9_78', '9_79', '9_80', '9_81', '9_82', '9_83', '9_84', '9_85', '9_86', '9_87', '9_88', '9_89', '9_90', '9_91', '9_92', '9_93', '9_94', '9_95', '9_96', '9_97', '9_98', '9_99', '9_100', '9_101', '9_102', '9_103', '9_104', '9_105', '9_106', '9_107', '9_108', '9_109', '9_110', '9_111', '9_112', '9_113', '9_114', '9_115', '9_116', '9_117', '9_118', '9_119', '9_120', '9_121', '9_122', '9_123', '9_124', '9_125', '9_126', '9_127', '9_128', '9_129', '9_130', '9_131', '9_132', '9_133', '9_134', '9_135', '9_136', '9_137', '9_138', '9_139', '9_140', '9_141', '9_142', '9_143', '9_144', '9_145', '9_146', '9_147', '9_148', '9_149', '9_150', '9_151', '9_152', '9_153', '9_154', '9_155', '9_156', '9_157', '9_158', '9_159', '9_160', '9_161', '9_162', '9_163', '9_164', '9_165', '9_166', '9_167', '9_168', '9_169', '9_170', '9_171', '9_172', '9_173', '9_174', '9_175', '9_176', '9_177', '9_178', '9_179', '9_180', '9_181', '9_182', '9_183', '9_184', '9_185', '9_186', '9_187', '9_188', '9_189', '9_190', '9_191', '9_192', '9_193', '9_194', '9_195', '9_196', '9_197', '9_198', '9_199', '9_200', '9_201', '9_202', '9_203', '9_204', '9_205', '9_206', '9_207', '9_208', '9_209', '9_210', '9_211', '9_212', '9_213', '9_214', '9_215', '9_216', '9_217', '9_218', '9_219', '9_220', '9_221', '9_222', '9_223', '9_224', '9_225', '9_226', '9_227', '9_228', '9_229', '9_230', '9_231', '9_232', '9_233', '9_234', '9_235', '9_236', '9_237', '9_238', '9_239', '9_240', '9_241', '9_242', '9_243', '9_244', '9_245', '9_246', '9_247', '9_248', '9_249', '9_250', '9_251', '9_252', '9_253', '9_254', '9_255', '9_256', '9_257', '9_258', '9_259', '9_260', '9_261', '9_262', '9_263', '9_264', '9_265', '9_266', '9_267', '9_268', '9_269', '9_270', '9_271', '9_272', '9_273', '9_274', '9_275', '9_276', '9_277', '9_278', '9_279', '9_280', '9_281', '9_282', '9_283', '9_284', '9_285', '9_286', '9_287', '9_288', '9_289', '9_290', '9_291', '9_292', '9_293', '9_294', '9_295', '9_296', '9_297', '9_298', '9_299', '9_300', '9_301', '9_302', '9_303', '9_304', '9_305', '9_306', '9_307', '9_308', '9_309', '9_310', '9_311', '9_312', '9_313', '9_314', '9_315', '9_316', '9_317', '9_318', '9_319', '9_320', '9_321', '9_322', '9_323', '9_324', '9_325', '9_326', '9_327', '9_328', '9_329', '9_330', '9_331', '9_332', '9_333', '9_334', '9_335', '9_336', '9_337', '9_338', '9_339', '9_340', '9_341', '9_342', '9_343', '9_344', '9_345', '9_346', '9_347', '9_348', '9_349', '9_350', '9_351', '9_352', '9_353', '9_354', '9_355', '9_356', '9_357', '9_358', '9_359', '9_360', '9_361', '9_362', '9_363', '9_364', '9_365', '9_366', '9_367', '9_368', '9_369', '9_370', '9_371', '20_0', '20_1', '20_2', '20_3', '20_4', '20_5', '20_6', '16_0', '16_1', '16_2', '16_3', '16_4', '16_5', '16_6', '16_7', '16_8', '16_9', '16_10', '16_11', '16_12', '16_13', '16_14', '16_15', '16_16', '16_17', '16_18', '16_19', '16_20', '16_21', '16_22', '16_23', '16_24', '16_25', '16_26', '16_27', '16_28', '16_29', '16_30', '16_31', '16_32', '16_33', '16_34', '16_35', '16_36', '16_37', '16_38', '16_39', '16_40', '16_41', '14_0', '14_1', '14_2', '14_3', '14_4', '14_5', '14_6', '14_7', '14_8', '14_9', '14_10', '14_11', '14_12', '14_13', '14_14', '14_15', '14_16', '14_17', '14_18', '14_19', '14_20', '14_21', '14_22', '14_23', '14_24', '14_25', '14_26', '14_27', '14_28', '14_29', '14_30', '14_31', '14_32', '14_33', '14_34', '14_35', '14_36', '14_37', '14_38', '14_39', '14_40', '14_41', '14_42', '14_43', '14_44', '14_45', '14_46', '14_47', '14_48', '14_49', '14_50', '14_51', '14_52', '14_53', '14_54', '14_55', '14_56', '14_57', '14_58', '14_59', '14_60', '14_61', '14_62', '14_63', '14_64', '14_65', '14_66', '14_67', '14_68', '14_69', '14_70', '14_71', '21_0', '21_1', '21_2', '21_3', '21_4', '21_5', '18_0', '18_1', '18_2', '18_3', '18_4', '18_5', '18_6', '18_7', '18_8', '18_9', '18_10', '18_11', '7_0', '7_1', '7_2', '7_3', '7_4', '7_5', '7_6', '7_7', '7_8', '7_9', '7_10', '7_11', '7_12', '7_13', '7_14', '7_15', '7_16', '7_17', '7_18', '7_19', '7_20', '7_21', '7_22', '7_23', '7_24', '7_25', '7_26', '7_27', '7_28', '7_29', '7_30', '7_31', '7_32', '7_33', '7_34', '7_35', '7_36', '7_37', '7_38', '7_39', '7_40', '7_41', '7_42', '7_43', '7_44', '7_45', '7_46', '7_47', '7_48', '7_49', '7_50', '7_51', '7_52', '7_53', '7_54', '7_55', '7_56', '7_57', '7_58', '7_59', '7_60', '7_61', '7_62', '7_63', '7_64', '7_65', '7_66', '7_67', '7_68', '7_69', '7_70', '7_71', '7_72', '7_73', '7_74', '7_75', '7_76', '7_77', '7_78', '7_79', '7_80', '7_81', '7_82', '7_83', '7_84', '7_85', '7_86', '7_87', '7_88', '7_89', '7_90', '7_91', '7_92', '7_93', '7_94', '7_95', '7_96', '7_97', '7_98', '7_99', '7_100', '7_101', '7_102', '7_103', '7_104', '7_105', '7_106', '7_107', '7_108', '7_109', '7_110', '7_111', '7_112', '7_113', '7_114', '7_115', '7_116', '7_117', '7_118', '7_119', '7_120', '7_121', '7_122', '7_123', '7_124', '7_125', '7_126', '7_127', '7_128', '7_129', '7_130', '7_131', '7_132', '7_133', '7_134', '7_135', '7_136', '7_137', '7_138', '7_139', '7_140', '7_141', '7_142', '7_143', '7_144', '7_145', '7_146', '7_147', '7_148', '7_149', '7_150', '7_151', '7_152', '7_153', '7_154', '7_155', '7_156', '7_157', '7_158', '7_159', '7_160', '7_161', '7_162', '7_163', '7_164', '7_165', '7_166', '7_167', '7_168', '7_169', '7_170', '7_171', '7_172', '7_173', '7_174', '7_175', '7_176', '7_177', '7_178', '7_179', '7_180', '7_181', '7_182', '7_183', '7_184', '7_185', '7_186', '7_187', '7_188', '7_189', '7_190', '7_191', '7_192', '7_193', '7_194', '7_195', '7_196', '7_197', '7_198', '7_199', '7_200', '7_201', '7_202', '7_203', '7_204', '7_205', '7_206', '7_207', '7_208', '7_209', '7_210', '7_211', '7_212', '7_213', '7_214', '7_215', '7_216', '7_217', '7_218', '7_219', '7_220', '7_221', '7_222', '7_223', '7_224', '7_225', '7_226', '7_227', '7_228', '7_229', '7_230', '7_231', '7_232', '7_233', '7_234', '7_235', '7_236', '7_237', '7_238', '7_239', '7_240', '7_241', '7_242', '7_243', '7_244', '7_245', '7_246', '7_247', '7_248', '7_249', '7_250', '7_251', '7_252', '7_253', '7_254', '7_255', '7_256', '7_257', '7_258', '7_259', '7_260', '7_261', '7_262', '7_263', '7_264', '7_265', '7_266', '7_267', '7_268', '7_269', '7_270', '7_271', '7_272', '7_273', '7_274', '7_275', '7_276', '7_277', '7_278', '7_279', '7_280', '7_281', '7_282', '7_283', '7_284', '7_285', '7_286', '7_287', '7_288', '7_289', '22_0', '22_1', '22_2', '22_3', '22_4', '26_0', '26_1', '24_0', '24_1', '24_2', '5_0', '5_1', '5_2', '5_3', '5_4', '5_5', '5_6', '5_7', '5_8', '5_9', '5_10', '5_11', '5_12', '5_13', '5_14', '5_15', '5_16', '5_17', '5_18', '5_19', '5_20', '5_21', '5_22', '5_23', '5_24', '5_25', '5_26', '5_27', '5_28', '5_29', '5_30', '5_31', '5_32', '5_33', '5_34', '5_35', '5_36', '5_37', '5_38', '5_39', '5_40', '5_41', '5_42', '5_43', '5_44', '5_45', '5_46', '5_47', '5_48', '5_49', '5_50', '5_51', '5_52', '5_53', '5_54', '5_55', '5_56', '5_57', '5_58', '5_59', '5_60', '5_61', '5_62', '5_63', '5_64', '5_65', '5_66', '5_67', '5_68', '5_69', '5_70', '5_71', '5_72', '5_73', '5_74', '5_75', '5_76', '5_77', '5_78', '5_79', '5_80', '5_81', '5_82', '5_83', '5_84', '5_85', '5_86', '5_87', '5_88', '5_89', '5_90', '5_91', '5_92', '5_93', '5_94', '5_95', '5_96', '5_97', '5_98', '5_99', '5_100', '5_101', '5_102', '5_103', '5_104', '5_105', '5_106', '5_107', '5_108', '5_109', '5_110', '5_111', '5_112', '5_113', '5_114', '5_115', '5_116', '5_117', '5_118', '5_119', '5_120', '5_121', '5_122', '5_123', '5_124', '5_125', '5_126', '5_127', '5_128', '5_129', '5_130', '5_131', '5_132', '5_133', '5_134', '5_135', '5_136', '5_137', '5_138', '5_139', '5_140', '8_0', '8_1', '8_2', '8_3', '8_4', '8_5', '8_6', '8_7', '8_8', '8_9', '8_10', '8_11', '8_12', '8_13', '8_14', '8_15', '8_16', '8_17', '8_18', '8_19', '8_20', '8_21', '8_22', '8_23', '8_24', '8_25', '8_26', '8_27', '8_28', '8_29', '8_30', '8_31', '8_32', '8_33', '8_34', '8_35', '8_36', '8_37', '8_38', '8_39', '8_40', '8_41', '8_42', '8_43', '8_44', '8_45', '8_46', '8_47', '8_48', '8_49', '8_50', '8_51', '8_52', '8_53', '8_54', '8_55', '8_56', '8_57', '8_58', '8_59', '8_60', '8_61', '8_62', '8_63', '8_64', '8_65', '8_66', '8_67', '8_68', '8_69', '8_70', '8_71', '8_72', '8_73', '8_74', '8_75', '8_76', '8_77', '8_78', '8_79', '8_80', '8_81', '8_82', '8_83', '8_84', '8_85', '8_86', '8_87', '8_88', '8_89', '8_90', '8_91', '8_92', '8_93', '8_94', '8_95', '8_96', '8_97', '8_98', '8_99', '8_100', '8_101', '8_102', '8_103', '8_104', '8_105', '8_106', '8_107', '8_108', '8_109', '8_110', '8_111', '8_112', '8_113', '8_114', '8_115', '8_116', '8_117', '8_118', '8_119', '8_120', '8_121', '8_122', '8_123', '8_124', '8_125', '8_126', '8_127', '8_128', '8_129', '8_130', '8_131', '8_132', '8_133', '8_134', '8_135', '8_136', '8_137', '8_138', '8_139', '8_140', '8_141', '8_142', '8_143', '8_144', '8_145', '8_146', '8_147', '8_148', '8_149', '8_150', '8_151', '8_152', '8_153', '8_154', '8_155', '8_156', '8_157', '8_158', '8_159', '8_160', '8_161', '8_162', '8_163', '8_164', '8_165', '8_166', '8_167', '8_168', '8_169', '8_170', '8_171', '8_172', '8_173', '8_174', '8_175', '8_176', '8_177', '8_178', '8_179', '8_180', '8_181', '8_182', '8_183', '8_184', '8_185', '8_186', '8_187', '8_188', '8_189', '8_190', '8_191', '8_192', '8_193', '8_194', '8_195', '8_196', '8_197', '8_198', '8_199', '8_200', '8_201', '8_202', '8_203', '8_204', '8_205', '8_206', '8_207', '8_208', '8_209', '8_210', '8_211', '8_212', '8_213', '8_214', '8_215', '8_216', '8_217', '8_218', '8_219', '8_220', '8_221', '8_222', '8_223', '8_224', '8_225', '8_226', '8_227', '8_228', '8_229', '8_230', '8_231', '8_232', '8_233', '8_234', '8_235', '8_236', '8_237', '8_238', '8_239', '8_240', '8_241', '8_242', '8_243', '8_244', '8_245', '8_246', '8_247', '8_248', '8_249', '8_250', '8_251', '8_252', '8_253', '8_254', '8_255', '8_256', '8_257', '8_258', '8_259', '8_260', '8_261', '8_262', '8_263', '8_264', '8_265', '8_266', '8_267', '8_268', '8_269', '8_270', '8_271', '8_272', '8_273', '8_274', '8_275', '8_276', '8_277', '8_278', '8_279', '8_280', '8_281', '8_282', '8_283', '8_284', '8_285', '8_286', '8_287', '8_288', '8_289', '8_290', '8_291', '8_292', '8_293', '8_294', '8_295', '8_296', '8_297', '8_298', '8_299', '8_300', '8_301', '8_302', '8_303', '8_304', '8_305', '8_306', '8_307', '8_308', '8_309', '8_310', '8_311', '8_312', '8_313', '8_314', '8_315', '8_316', '8_317', '8_318', '8_319', '8_320', '8_321', '8_322', '8_323', '8_324', '8_325', '8_326', '8_327', '8_328', '8_329', '8_330', '8_331', '8_332', '8_333', '8_334', '8_335', '8_336', '8_337', '8_338', '8_339', '8_340', '8_341', '8_342', '8_343', '8_344', '8_345', '8_346', '8_347', '8_348', '8_349', '8_350', '8_351', '8_352', '8_353', '8_354', '8_355', '8_356', '8_357', '8_358', '8_359', '8_360', '8_361', '8_362', '8_363', '8_364', '8_365', '8_366', '8_367', '8_368', '8_369', '8_370', '8_371', '8_372', '8_373', '8_374', '8_375', '8_376', '8_377', '8_378', '8_379', '8_380', '8_381', '8_382', '8_383', '8_384', '8_385', '8_386', '8_387', '8_388', '8_389', '8_390', '8_391', '8_392', '8_393', '8_394', '8_395', '8_396', '8_397', '8_398', '8_399', '8_400', '8_401', '8_402', '8_403', '8_404', '8_405', '8_406', '8_407', '8_408', '8_409', '8_410', '8_411', '8_412', '8_413', '8_414', '8_415', '6_0', '6_1', '6_2', '6_3', '6_4', '6_5', '6_6', '6_7', '6_8', '6_9', '6_10', '6_11', '6_12', '6_13', '6_14', '6_15', '6_16', '6_17', '6_18', '6_19', '6_20', '6_21', '6_22', '6_23', '6_24', '6_25', '6_26', '6_27', '6_28', '6_29', '6_30', '6_31', '6_32', '6_33', '6_34', '6_35', '6_36', '6_37', '6_38', '6_39', '6_40', '6_41', '6_42', '6_43', '6_44', '6_45', '6_46', '6_47', '6_48', '6_49', '6_50', '6_51', '6_52', '6_53', '6_54', '6_55', '6_56', '6_57', '6_58', '6_59', '6_60', '6_61', '6_62', '6_63', '6_64', '6_65', '6_66', '6_67', '6_68', '6_69', '6_70', '6_71', '6_72', '6_73', '6_74', '6_75', '6_76', '6_77', '6_78', '6_79', '6_80', '6_81', '6_82', '6_83', '6_84', '6_85', '6_86', '6_87', '6_88', '6_89', '6_90', '6_91', '6_92', '6_93', '6_94', '6_95', '6_96', '6_97', '6_98', '6_99', '6_100', '6_101', '6_102', '6_103', '6_104', '6_105', '6_106', '6_107', '6_108', '6_109', '6_110', '6_111', '6_112', '6_113', '6_114', '6_115', '6_116', '6_117', '6_118', '6_119', '6_120', '6_121', '6_122', '6_123', '6_124', '6_125', '6_126', '6_127', '6_128', '6_129', '6_130', '6_131', '6_132', '6_133', '6_134', '6_135', '6_136', '6_137', '6_138', '6_139', '6_140', '6_141', '6_142', '6_143', '6_144', '6_145', '6_146', '6_147', '6_148', '6_149', '6_150', '6_151', '6_152', '6_153', '6_154', '6_155', '6_156', '6_157', '6_158', '6_159', '6_160', '6_161', '6_162', '6_163', '6_164', '6_165', '6_166', '6_167', '6_168', '6_169', '6_170', '6_171', '6_172', '6_173', '6_174', '6_175', '6_176', '6_177', '6_178', '6_179', '6_180', '6_181', '6_182', '6_183', '6_184', '6_185', '6_186', '6_187', '6_188', '6_189', '6_190', '6_191', '6_192', '6_193', '6_194', '6_195', '6_196', '6_197', '6_198', '6_199', '6_200', '6_201', '6_202', '6_203', '6_204', '6_205', '6_206', '6_207', '6_208', '6_209', '6_210', '6_211', '6_212', '6_213', '6_214', '6_215', '6_216', '6_217', '6_218', '6_219', '6_220', '6_221', '6_222', '6_223', '6_224', '6_225', '6_226', '6_227', '6_228', '6_229', '6_230', '6_231', '6_232', '6_233', '6_234', '6_235', '6_236', '6_237', '6_238', '6_239', '6_240', '6_241', '6_242', '6_243', '6_244', '6_245', '6_246', '6_247', '6_248', '6_249', '6_250', '6_251', '6_252', '6_253', '6_254', '6_255', '6_256', '6_257', '6_258', '6_259', '6_260', '6_261', '6_262', '6_263', '6_264', '6_265', '6_266', '6_267', '6_268', '6_269', '6_270', '6_271', '6_272', '6_273', '6_274', '6_275', '6_276', '6_277', '6_278', '6_279', '6_280', '6_281', '6_282', '6_283', '6_284', '6_285', '6_286', '6_287', '6_288', '6_289', '6_290', '6_291', '6_292', '6_293', '6_294', '6_295', '6_296', '6_297', '6_298', '6_299', '6_300', '6_301', '6_302', '6_303', '6_304', '6_305', '6_306', '6_307', '6_308', '6_309', '6_310', '6_311', '6_312', '6_313', '6_314', '6_315', '6_316', '6_317', '6_318', '6_319', '6_320', '6_321', '6_322', '6_323', '6_324', '6_325', '6_326', '6_327', '6_328', '6_329', '6_330', '6_331', '6_332', '6_333', '6_334', '6_335', '6_336', '6_337', '6_338', '6_339', '6_340', '6_341', '6_342', '6_343', '6_344', '6_345', '6_346', '6_347', '6_348', '6_349', '6_350', '6_351', '6_352', '4_0', '4_1', '4_2', '4_3', '4_4', '4_5', '4_6', '4_7', '4_8', '4_9', '4_10', '4_11', '4_12', '4_13', '4_14', '4_15', '4_16', '4_17', '4_18', '4_19', '4_20', '4_21', '4_22', '4_23', '4_24', '4_25', '4_26', '4_27', '4_28', '4_29', '4_30', '4_31', '4_32', '4_33', '4_34', '4_35', '4_36', '4_37', '4_38', '4_39', '4_40', '4_41', '4_42', '4_43', '4_44', '4_45', '4_46', '4_47', '4_48', '4_49', '4_50', '4_51', '4_52', '4_53', '4_54', '4_55', '4_56', '4_57', '4_58', '4_59', '4_60', '4_61', '4_62', '4_63', '4_64', '4_65', '4_66', '4_67', '4_68', '4_69', '4_70', '4_71', '4_72', '4_73', '4_74', '3_0', '3_1', '3_2', '3_3', '3_4', '3_5', '3_6', '3_7', '3_8', '3_9', '3_10', '3_11', '3_12', '3_13', '3_14', '3_15', '3_16', '3_17', '25_0', '25_1', '25_2', '25_3', '23_0', '23_1', '23_2', '23_3', '30_0', '29_0', '28_0', '27_0', '27_1', '31_0', '32_0', '33_0', '36_0', '39_0', '34_0', '35_0', '37_0'])"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(a['inputs'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
