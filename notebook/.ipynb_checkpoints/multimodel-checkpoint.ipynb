{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pickle.load(open('./multi_train_data', 'rb'))\n",
    "label_all = pickle.load(open('./multi_train_label', 'rb'))\n",
    "test_data = pickle.load(open('./multi_test_data', 'rb'))\n",
    "test_label_all = pickle.load(open('./multi_test_label', 'rb'))\n",
    "sizes = pickle.load(open('./multi_label_sizes', 'rb'))\n",
    "rebuild_label = pickle.load(open('./rebuild_label.npz', 'rb'))\n",
    "\n",
    "data_all = {\n",
    "    'train_inputs': data,\n",
    "    'train_targets': label_all,\n",
    "    'valid_inputs': test_data,\n",
    "    'valid_targets': test_label_all,\n",
    "    'label_size': sizes\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class parameter(object):\n",
    "    \n",
    "    def __init__(self, shape):\n",
    "        self.shape = shape\n",
    "    \n",
    "    def normal(self):\n",
    "        return tf.Variable(tf.random_normal(self.shape))\n",
    "\n",
    "class layer(object):\n",
    "    \n",
    "    def __init__(self, shape):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def __str__(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    \n",
    "    \n",
    "class lstm_layer(layer):\n",
    "    \n",
    "    def __init__(self, inputs, n_units, scope):\n",
    "        \n",
    "        self.n_units = n_units\n",
    "        self.input_shape = inputs.shape\n",
    "        self.outputs, self.states = tf.nn.dynamic_rnn(\n",
    "            cell = tf.contrib.rnn.BasicLSTMCell(self.n_units),\n",
    "            inputs = inputs,\n",
    "            dtype = tf.float32,\n",
    "            scope = scope)\n",
    "        \n",
    "    def __str__(self):\n",
    "        print('|---------------[LSTM layer]---------------|')\n",
    "        print('shape: ', self.input_shape, ' => ', self.outputs.shape)\n",
    "        return '|_______________[LSTM layer]_______________|'\n",
    "        \n",
    "class affine_layer(layer):\n",
    "    \n",
    "    def __init__(self, inputs, shape, activation):\n",
    "        \n",
    "        self.shape = shape\n",
    "        self.input_shape = inputs.shape\n",
    "        self.activation = activation\n",
    "        self.weights = parameter(self.shape).normal()\n",
    "        self.biases = parameter([self.shape[-1]]).normal()\n",
    "        self.outputs = self.activation(tf.add(tf.matmul(inputs, self.weights), self.biases))\n",
    "\n",
    "    def __str__(self):\n",
    "        print('|---------------[Affine layer]---------------|')\n",
    "        print('shape: ', self.input_shape, ' => ', self.outputs.shape)\n",
    "        return '|_______________[Affine layer]_______________|'\n",
    "    \n",
    "class reshape_layer(layer):\n",
    "    \n",
    "    def __init__(self, inputs, shape):\n",
    "        \n",
    "        self.input_shape = inputs.shape\n",
    "        self.outputs = tf.reshape(inputs, shape)\n",
    "\n",
    "    def __str__(self):\n",
    "        print('|---------------[Reshape layer]---------------|')\n",
    "        print('shape: ', self.input_shape, ' => ', self.outputs.shape)\n",
    "        return '|_______________[Reshape layer]_______________|'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([11, 9, 17, 13, 10, 8, 15, 7, 18, 14, 12, 19, 16, 5, 20, 24, 22, 3, 6, 4, 2, 1, 23, 21, 25, 28, 27, 26, 29, 30, 31, 33, 34, 37, 32, 35])\n"
     ]
    }
   ],
   "source": [
    "keys = data.keys()\n",
    "print(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134000\n",
      "Training Epoch: 0, acc: 0.000037, loss: 0.008072 \n",
      "13400\n",
      "-----------------------------------------------------------------\n",
      "Testing Epoch: 0, acc: 0.000075, loss: 0.005841 \n",
      "-----------------------------------------------------------------\n",
      "134000\n",
      "Training Epoch: 1, acc: 0.186746, loss: 0.000201 \n",
      "134000\n",
      "Training Epoch: 2, acc: 0.150448, loss: 0.000177 \n",
      "134000\n",
      "Training Epoch: 3, acc: 0.140142, loss: 0.000175 \n",
      "134000\n",
      "Training Epoch: 4, acc: 0.140075, loss: 0.000180 \n",
      "134000\n",
      "Training Epoch: 5, acc: 0.116172, loss: 0.000176 \n",
      "13400\n",
      "-----------------------------------------------------------------\n",
      "Testing Epoch: 5, acc: 0.403806, loss: 0.001712 \n",
      "-----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "learning_rate = 1e-3\n",
    "iteration = 100\n",
    "interval = 5\n",
    "keys = data.keys()\n",
    "keys_valid = test_data.keys()\n",
    "with graph.as_default():\n",
    "    \n",
    "    with tf.name_scope('multi_model1'):\n",
    "        \n",
    "        with tf.name_scope('placeholder'):\n",
    "            with tf.name_scope('input_placeholder'):\n",
    "                input_placeholder = tf.placeholder(tf.float32, [None, None, 16])\n",
    "\n",
    "            with tf.name_scope('target_placeholder_part0'):\n",
    "                target_placeholder_p0 = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "            with tf.name_scope('target_placeholder_part1'):\n",
    "                target_placeholder_p1 = tf.placeholder(tf.float32, [None, 75])\n",
    "\n",
    "            with tf.name_scope('target_placeholder_part2'):\n",
    "                target_placeholder_p2 = tf.placeholder(tf.float32, [None, 38])\n",
    "\n",
    "            with tf.name_scope('target_placeholder_part3'):\n",
    "                target_placeholder_p3 = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "            with tf.name_scope('target_placeholder_part4'):\n",
    "                target_placeholder_p4 = tf.placeholder(tf.float32, [None, 2])\n",
    "\n",
    "            with tf.name_scope('target_placeholder_rebuild'):\n",
    "                target_placeholder_rebuild = tf.placeholder(tf.float32, [None, 134])\n",
    "\n",
    "        with tf.name_scope('model_0'):\n",
    "            model_0_lstm_0 = lstm_layer(input_placeholder, 16, 'model_0')\n",
    "            model_0_reshape_0 = reshape_layer(model_0_lstm_0.outputs[:,-1,:], [-1, 16])\n",
    "            model_0_affine_0 = affine_layer(model_0_reshape_0.outputs, [16, 256], tf.nn.relu)\n",
    "            model_0_output = affine_layer(model_0_affine_0.outputs, [256, 10], tf.identity)\n",
    "            tf.summary.image('model_0_classify_weights', reshape_layer(model_0_output.weights, [-1,256,10,1]).outputs)\n",
    "        with tf.name_scope('model_1'):\n",
    "            model_1_lstm_0 = lstm_layer(input_placeholder, 16, 'model_1')\n",
    "            model_1_reshape_0 = reshape_layer(model_1_lstm_0.outputs[:,-1,:], [-1, 16])\n",
    "            model_1_affine_0 = affine_layer(model_1_reshape_0.outputs, [16, 256], tf.nn.relu)\n",
    "            model_1_output = affine_layer(model_1_affine_0.outputs, [256, 75], tf.identity)\n",
    "            tf.summary.image('model_1_classify_weights', reshape_layer(model_1_output.weights, [-1,256,75,1]).outputs)\n",
    "        with tf.name_scope('model_2'):\n",
    "            model_2_lstm_0 = lstm_layer(input_placeholder, 16, 'model_2')\n",
    "            model_2_reshape_0 = reshape_layer(model_2_lstm_0.outputs[:,-1,:], [-1, 16])\n",
    "            model_2_affine_0 = affine_layer(model_2_reshape_0.outputs, [16, 256], tf.nn.relu)\n",
    "            model_2_output = affine_layer(model_2_affine_0.outputs, [256, 38], tf.identity)\n",
    "            tf.summary.image('model_2_classify_weights', reshape_layer(model_2_output.weights, [-1,256,38,1]).outputs)\n",
    "        with tf.name_scope('model_3'):\n",
    "            model_3_lstm_0 = lstm_layer(input_placeholder, 16, 'model_3')\n",
    "            model_3_reshape_0 = reshape_layer(model_3_lstm_0.outputs[:,-1,:], [-1, 16])\n",
    "            model_3_affine_0 = affine_layer(model_3_reshape_0.outputs, [16, 256], tf.nn.relu)\n",
    "            model_3_output = affine_layer(model_3_affine_0.outputs, [256, 10], tf.identity)\n",
    "            tf.summary.image('model_3_classify_weights', reshape_layer(model_3_output.weights, [-1,256,10,1]).outputs)\n",
    "        with tf.name_scope('model_4'):\n",
    "            model_4_lstm_0 = lstm_layer(input_placeholder, 16, 'model_4')\n",
    "            model_4_reshape_0 = reshape_layer(model_4_lstm_0.outputs[:,-1,:], [-1, 16])\n",
    "            model_4_affine_0 = affine_layer(model_4_reshape_0.outputs, [16, 256], tf.nn.relu)\n",
    "            model_4_output = affine_layer(model_4_affine_0.outputs, [256, 2], tf.identity)\n",
    "            tf.summary.image('model_4_classify_weights', reshape_layer(model_4_output.weights, [-1,256,2,1]).outputs)\n",
    "        \n",
    "        with tf.name_scope('combined_model'):\n",
    "            combine = tf.concat([tf.sigmoid(model_0_affine_0.outputs), \n",
    "                                 tf.sigmoid(model_1_affine_0.outputs),\n",
    "                                 tf.sigmoid(model_2_affine_0.outputs),\n",
    "                                 tf.sigmoid(model_3_affine_0.outputs),\n",
    "                                 tf.sigmoid(model_4_affine_0.outputs)], 1)\n",
    "#             print(combine)\n",
    "            tf.summary.image('combine', reshape_layer(combine, [1, -1, 1280, 1]).outputs)\n",
    "#             reschedule = tf.sigmoid(combine)\n",
    "#             print(reschedule)\n",
    "            affine0 = affine_layer(combine, [1280, 256], tf.nn.relu)\n",
    "            output = affine_layer(affine0.outputs, [256, 134], tf.identity)\n",
    "        \n",
    "        with tf.name_scope('result_0'):\n",
    "            model_0_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=model_0_output.outputs, labels=target_placeholder_p0))\n",
    "            model_0_optimizer = tf.train.AdamOptimizer(learning_rate).minimize(model_0_loss)\n",
    "            model_0_accuracy = tf.reduce_sum(tf.cast(tf.equal(tf.argmax(model_0_output.outputs, 1), tf.argmax(target_placeholder_p0, 1)), tf.float32))\n",
    "            tf.summary.scalar('model_0_accuracy', model_0_accuracy)\n",
    "        with tf.name_scope('result_1'):\n",
    "            model_1_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=model_1_output.outputs, labels=target_placeholder_p1))\n",
    "            model_1_optimizer = tf.train.AdamOptimizer(learning_rate).minimize(model_1_loss)\n",
    "            model_1_accuracy = tf.reduce_sum(tf.cast(tf.equal(tf.argmax(model_1_output.outputs, 1), tf.argmax(target_placeholder_p1, 1)), tf.float32))\n",
    "            tf.summary.scalar('model_1_accuracy', model_1_accuracy)\n",
    "        with tf.name_scope('result_2'):\n",
    "            model_2_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=model_2_output.outputs, labels=target_placeholder_p2))\n",
    "            model_2_optimizer = tf.train.AdamOptimizer(learning_rate).minimize(model_2_loss)\n",
    "            model_2_accuracy = tf.reduce_sum(tf.cast(tf.equal(tf.argmax(model_2_output.outputs, 1), tf.argmax(target_placeholder_p2, 1)), tf.float32))\n",
    "            tf.summary.scalar('model_2_accuracy', model_2_accuracy)\n",
    "        with tf.name_scope('result_3'):\n",
    "            model_3_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=model_3_output.outputs, labels=target_placeholder_p3))\n",
    "            model_3_optimizer = tf.train.AdamOptimizer(learning_rate).minimize(model_3_loss)\n",
    "            model_3_accuracy = tf.reduce_sum(tf.cast(tf.equal(tf.argmax(model_3_output.outputs, 1), tf.argmax(target_placeholder_p3, 1)), tf.float32))\n",
    "            tf.summary.scalar('model_3_accuracy', model_3_accuracy)\n",
    "        with tf.name_scope('result_4'):\n",
    "            model_4_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=model_4_output.outputs, labels=target_placeholder_p4))\n",
    "            model_4_optimizer = tf.train.AdamOptimizer(learning_rate).minimize(model_4_loss)\n",
    "            model_4_accuracy = tf.reduce_sum(tf.cast(tf.equal(tf.argmax(model_4_output.outputs, 1), tf.argmax(target_placeholder_p4, 1)), tf.float32))\n",
    "            tf.summary.scalar('model_4_accuracy', model_4_accuracy)\n",
    "        with tf.name_scope('result_all'):\n",
    "            loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=output.outputs, labels=target_placeholder_rebuild))\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "            accuracy = tf.reduce_sum(tf.cast(tf.equal(tf.argmax(output.outputs, 1), tf.argmax(target_placeholder_rebuild, 1)), tf.float32))\n",
    "            tf.summary.scalar('model_accuracy', accuracy)\n",
    "            \n",
    "        sess = tf.Session()\n",
    "        merged = tf.summary.merge_all()\n",
    "        writer = tf.summary.FileWriter('./output', sess.graph)\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        with tf.name_scope('train'):\n",
    "            \n",
    "            for i in range(iteration):\n",
    "                accs = 0.0\n",
    "                errs = 0.0\n",
    "                ns = 0\n",
    "                for key in keys:\n",
    "                    \n",
    "                    feed_dict = {\n",
    "                        input_placeholder: data[key],\n",
    "                        target_placeholder_p0: label_all[0][key],\n",
    "                        target_placeholder_p1: label_all[1][key],\n",
    "                        target_placeholder_p2: label_all[2][key],\n",
    "                        target_placeholder_p3: label_all[3][key],\n",
    "                        target_placeholder_p4: label_all[4][key],\n",
    "                        target_placeholder_rebuild: rebuild_label['train_targets'][key]\n",
    "                    }\n",
    "                    ns += len(data[key])\n",
    "                    merge, _, acc, err = sess.run([merged, optimizer, accuracy, loss], feed_dict=feed_dict)\n",
    "                    writer.add_summary(merge, i)\n",
    "                    accs += acc\n",
    "                    errs += err\n",
    "                accs /= ns\n",
    "                errs /= ns\n",
    "#                 print(ns)\n",
    "                print('Training Epoch: %d, acc: %f, loss: %f '%(i, accs, errs))\n",
    "                if i % interval == 0:\n",
    "                    accs = 0.0\n",
    "                    errs = 0.0\n",
    "                    ns = 0\n",
    "                    for key in keys_valid:\n",
    "\n",
    "                        feed_dict = {\n",
    "                            input_placeholder: test_data[key],\n",
    "                            target_placeholder_p0: test_label_all[0][key],\n",
    "                            target_placeholder_p1: test_label_all[1][key],\n",
    "                            target_placeholder_p2: test_label_all[2][key],\n",
    "                            target_placeholder_p3: test_label_all[3][key],\n",
    "                            target_placeholder_p4: test_label_all[4][key],\n",
    "                            target_placeholder_rebuild: rebuild_label['valid_targets'][key]\n",
    "                        }\n",
    "                        ns += len(test_data[key])\n",
    "                        acc, err = sess.run([accuracy, loss], feed_dict=feed_dict)\n",
    "                        accs += acc\n",
    "                        errs += err\n",
    "                    accs /= ns\n",
    "                    errs /= ns\n",
    "#                     print(ns)\n",
    "                    print('-----------------------------------------------------------------')\n",
    "                    print('Testing Epoch: %d, acc: %f, loss: %f '%(i, accs, errs))\n",
    "                    print('-----------------------------------------------------------------')\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(np.array(data[11]).shape)\n",
    "for i in range(5):\n",
    "    print(np.array(label_all[i][11]).shape)\n",
    "print(rebuild_label['train_targets'][11].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([14, 15, 16, 11, 21, 12, 18, 13, 17, 10, 19, 20, 26, 8, 9, 5, 7, 6, 4, 3, 2, 1, 22, 23, 25, 24, 33, 27, 29, 28, 35])\n"
     ]
    }
   ],
   "source": [
    "print(test_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
