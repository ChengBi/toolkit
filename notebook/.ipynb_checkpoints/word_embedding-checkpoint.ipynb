{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class corpusClean():\n",
    "\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "        self.num_re = re.compile('[0-9一二三四五六七八九十]+')\n",
    "        self.char_re = re.compile('[a-zA-Z\\-\\,\\.\\。\\，]+')\n",
    "\n",
    "    def clean(self):\n",
    "        lines = open(self.filename).readlines()\n",
    "        cleaned = open('../data/cleaned_corpus.txt', 'w', encoding='utf-8')\n",
    "        for line in lines:\n",
    "            sentence = line.split()[-1]\n",
    "            removed_char = re.sub(self.char_re, ' <CHAR> ', sentence)\n",
    "            removed_num = re.sub(self.num_re, ' <NUMBER> ', removed_char)\n",
    "            words = removed_num.split()\n",
    "            saved_words = ['<BEGIN>']\n",
    "            for word in words:\n",
    "                if len(word) == 1 or word.find('<') != -1:\n",
    "                    saved_words.append(word)\n",
    "                else:\n",
    "                    for w in word:\n",
    "                        saved_words.append(w)\n",
    "            saved_words.append('<END>')\n",
    "            saved_sentence = ' '.join(saved_words)\n",
    "            cleaned.write(saved_sentence+'\\n')\n",
    "        cleaned.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class word2vector():\n",
    "\n",
    "    def __init__(self, filename, n_gram, read_flag):\n",
    "        self.filename = filename\n",
    "        self.n_gram = n_gram\n",
    "        self.lines = open(self.filename, encoding='utf-8').readlines()\n",
    "        self.lines = self.lines[:5000]\n",
    "        if read_flag == False:\n",
    "            self.encode()\n",
    "        else:\n",
    "            self.word_encoding = pickle.load(open('../data/word_encoding.npz', 'rb'))\n",
    "            self.word_map = pickle.load(open('../data/word_map.npz', 'rb'))\n",
    "            self.U = pickle.load(open('../data/U.npz', 'rb'))\n",
    "            self.S = pickle.load(open('../data/S.npz', 'rb'))\n",
    "            self.V = pickle.load(open('../data/V.npz', 'rb'))\n",
    "            \n",
    "    def encode(self):\n",
    "#         if use_svd == True:\n",
    "#             assert k!=None\n",
    "#         else:\n",
    "#             self.k = k\n",
    "        word_set = set()\n",
    "        for line in self.lines:\n",
    "            words = line.split()\n",
    "            for word in words:\n",
    "                word_set.add(word)\n",
    "        word_set.add('UNKNOWN')\n",
    "        word_set = list(word_set)\n",
    "        self.word_map = dict()\n",
    "        for i in range(len(word_set)):\n",
    "            self.word_map[word_set[i]] = i\n",
    "        self.word_encoding = np.identity(len(self.word_map))\n",
    "        # pickle.dump(self.word_encoding, open('../data/word_encoding.npz', 'wb'))\n",
    "        # self.word_encoding = np.zeros((len(self.word_map), len(self.word_map)))\n",
    "        for line in self.lines:\n",
    "            words = line.split()\n",
    "            for i in range(len(words)-self.n_gram):\n",
    "                neighbours = words[i:i+self.n_gram-1]+words[i+self.n_gram:i+2*self.n_gram-1]\n",
    "                for neighbour in neighbours:\n",
    "                    self.word_encoding[self.getIndex(words[i+self.n_gram-1])][self.getIndex(neighbour)] += 1\n",
    "        for i in range(self.word_encoding.shape[0]):\n",
    "            self.word_encoding[i] = self.word_encoding[i] / np.sum(self.word_encoding[i])\n",
    "#         print(self.word_encoding.shape)\n",
    "\n",
    "#         if use_svd == True:\n",
    "#             self.word_encoding_svd = self.SVD(self.word_encoding, 2000)\n",
    "#         else:\n",
    "#         self.word_encoding_svd = self.word_encoding\n",
    "\n",
    "        pickle.dump(self.word_encoding, open('../data/word_encoding.npz', 'wb'))\n",
    "        pickle.dump(self.word_map, open('../data/word_map.npz', 'wb'))\n",
    "\n",
    "    def decode(self, words):\n",
    "        result = np.zeros(len(self.word_map))\n",
    "        for word in words:\n",
    "            index = self.getIndex(word)\n",
    "            result[index] += 1\n",
    "        result = result/np.sum(result)\n",
    "#         print(result.shape)\n",
    "#         print(self.S.shape)\n",
    "#         result = np.dot(result, self.S)\n",
    "        return result\n",
    "\n",
    "    def decodeSentence(self, sentence):\n",
    "        words = sentence.split()\n",
    "        single_vector = np.zeros((len(words)-2, len(self.word_map)))\n",
    "        ngram_vector = np.zeros((len(words)-2, len(self.word_map)))\n",
    "        for i in range(len(words)-self.n_gram):\n",
    "            single_vector[i] = self.decode(words[i+self.n_gram-1])\n",
    "            neighbour = words[i:i+self.n_gram-1]+words[i+self.n_gram:i+2*self.n_gram-1]\n",
    "            ngram_vector[i] = self.decode(neighbour)\n",
    "        return single_vector, ngram_vector\n",
    "\n",
    "    def get(self):\n",
    "        self.data = dict()\n",
    "#         self.data['inputs'] = [None]*len(self.lines)\n",
    "#         self.data['targets'] = [None]*len(self.lines)\n",
    "        \n",
    "        self.data['inputs'] = dict()\n",
    "        self.data['targets'] = dict()\n",
    "        self.data['batch_size'] = dict()\n",
    "\n",
    "        for line, i in zip(self.lines, range(len(self.lines))):\n",
    "            single, ngram = self.decodeSentence(line)\n",
    "            size = single.shape[0]\n",
    "            if size not in self.data['batch_size'].keys():\n",
    "                self.data['inputs'][size] = []\n",
    "                self.data['targets'][size] = []\n",
    "                self.data['batch_size'][size] = 0\n",
    "            self.data['inputs'][size].append(single)\n",
    "            self.data['targets'][size].append(ngram)\n",
    "            self.data['batch_size'][size] += 1\n",
    "        \n",
    "        pickle.dump(self.data, open('../data/data.npz', 'wb'))\n",
    "        return self.data#, self.batches\n",
    "\n",
    "    def SVD(self, inputs, k):\n",
    "        self.U, self.S, self.V = np.linalg.svd(inputs)\n",
    "        self.U = self.U[:, :k]\n",
    "        self.S = np.diag(self.S)[:k, :k]\n",
    "        self.V = self.V[:k, :]\n",
    "        pickle.dump(self.U, open('../data/U.npz', 'wb'))\n",
    "        pickle.dump(self.S, open('../data/S.npz', 'wb'))\n",
    "        pickle.dump(self.V, open('../data/V.npz', 'wb'))\n",
    "        return self.U.dot(self.S)\n",
    "\n",
    "    def getIndex(self, word):\n",
    "        if word in self.word_map.keys():\n",
    "            return self.word_map[word]\n",
    "        else:\n",
    "            return self.word_map['UNKNOWN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = open('../data/cleaned_corpus.txt', encoding='utf-8').readlines()\n",
    "lines_set = set()\n",
    "for line in lines:\n",
    "    lines_set.add(line)\n",
    "print(len(lines_set))\n",
    "writer = open('../data/trimmed_cleaned_corpus.txt', 'w', encoding='utf-8')\n",
    "for line in list(lines_set):\n",
    "    writer.write(line)\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v = word2vector('../data/trimmed_cleaned_corpus.txt', 2, True)\n",
    "data = w2v.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for key in data['batch_size'].keys():\n",
    "#     print(np.array(data['inputs'][key]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
