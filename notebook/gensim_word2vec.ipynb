{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cheng.bi\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:862: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import re\n",
    "from tqdm import tqdm, tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class corpusClean():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.num_re = re.compile('[0-9一二三四五六七八九十]+')\n",
    "        self.char_re = re.compile('[a-zA-Z\\-\\,\\.\\。\\，]+')\n",
    "\n",
    "    def clean(self, line):\n",
    "        sentence = line\n",
    "        removed_char = re.sub(self.char_re, ' <CHAR> ', sentence)\n",
    "        removed_num = re.sub(self.num_re, ' <NUMBER> ', removed_char)\n",
    "        words = removed_num.split()\n",
    "        saved_words = ['<BEGIN>']\n",
    "        for word in words:\n",
    "            if len(word) == 1 or word.find('<') != -1:\n",
    "                saved_words.append(word)\n",
    "            else:\n",
    "                for w in word:\n",
    "                    saved_words.append(w)\n",
    "        saved_words.append('<END>')\n",
    "        return saved_words\n",
    "    \n",
    "    \n",
    "class layer(object):\n",
    "    def __init__(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def __str__(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "class affine_layer(layer):\n",
    "    def __init__(self, name, inputs, shape, activation, reuse=False):\n",
    "        with tf.variable_scope('affine', reuse=reuse):\n",
    "            self.weights = tf.get_variable(name = name+'_weights', initializer=tf.truncated_normal(shape, stddev=0.05), dtype=tf.float32)\n",
    "            self.bias = tf.get_variable(name = name+'_bias', initializer=tf.zeros([shape[-1]]))\n",
    "            self.outputs = activation(tf.add(tf.matmul(inputs, self.weights), self.bias))\n",
    "    def __str__(self):\n",
    "        return self.outputs.__str__()\n",
    "    \n",
    "class conv_layer(layer):\n",
    "    def __init__(self, name, inputs, shape, activation, reuse=False):\n",
    "        with tf.variable_scope('conv', reuse=reuse):\n",
    "            self.kernel = tf.get_variable(name = name+'_kernel', initializer=tf.truncated_normal(shape, stddev=0.05), dtype=tf.float32)\n",
    "            self.bias = tf.get_variable(name = name+'_bias', initializer=tf.zeros([shape[-1]]))\n",
    "            self.outputs = activation(tf.add(tf.nn.conv2d(inputs, self.kernel, padding='VALID', strides=[1,1,1,1]), self.bias))\n",
    "    def __str__(self):\n",
    "        return self.outputs.__str__()\n",
    "    \n",
    "class pooling_layer(layer):\n",
    "    def __init__(self, name, inputs, reuse=False):\n",
    "        with tf.variable_scope('pooling', reuse=reuse):\n",
    "            self.outputs = tf.nn.max_pool(name = name+'_maxpooling', value=inputs, ksize=[1,2,2,1], padding='VALID', strides=[1,2,2,1])\n",
    "    def __str__(self):\n",
    "        return self.outputs.__str__()\n",
    "        \n",
    "class reshape_layer(layer):\n",
    "    def __init__(self, name, inputs, shape, reuse=False):\n",
    "        with tf.variable_scope('reshape', reuse=reuse):\n",
    "            self.outputs = tf.reshape(name=name, tensor=inputs, shape=shape)\n",
    "    def __str__(self):\n",
    "        return self.outputs.__str__()\n",
    "        \n",
    "class deconv_layer(layer):\n",
    "    def __init__(self, name, inputs, kernel_shape, output_shape, activation, reuse=False):\n",
    "        with tf.variable_scope('deconv', reuse=reuse):\n",
    "            self.kernel = tf.get_variable(name = name+'_kernel', initializer=tf.truncated_normal(kernel_shape, stddev=0.05), dtype=tf.float32)\n",
    "            self.bias = tf.get_variable(name = name+'_bias', initializer=tf.zeros([kernel_shape[-2]]))\n",
    "            self.outputs = activation(tf.add(tf.nn.conv2d_transpose(inputs, self.kernel, output_shape=output_shape, padding='VALID', strides=[1,2,2,1]), self.bias))\n",
    "    def __str__(self):\n",
    "        return self.outputs.__str__()\n",
    "    \n",
    "class lstm_layer(layer):\n",
    "    def __init__(self, name, inputs, n_units, reuse=False):\n",
    "        with tf.variable_scope('lstm', reuse=reuse):\n",
    "            self.outputs, self.states = tf.nn.dynamic_rnn(\n",
    "                cell = tf.contrib.rnn.BasicLSTMCell(n_units),\n",
    "                inputs = inputs,\n",
    "                dtype = tf.float32)\n",
    "    def __str__(self):\n",
    "        return self.outputs.__str__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = open('../data/cleaned_corpus.txt', encoding='utf-8').readlines()\n",
    "lines = open('../data/TrainSet-eCarX-171019.txt', encoding='gbk').readlines()\n",
    "vector_size = 200\n",
    "ngram = 3\n",
    "min_count = 2\n",
    "workers = 4\n",
    "models = gensim.models.Word2Vec(sentences, size=vector_size, window=ngram, min_count=min_count, workers=workers)\n",
    "cleaner = corpusClean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models.save('../word2vec/word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = dict()\n",
    "train_target = dict()\n",
    "for sentence, line in zip(sentences, lines):\n",
    "    words = sentence.split()\n",
    "    vector = np.zeros((len(words), vector_size))\n",
    "    for word,i in zip(words, range(len(words))):\n",
    "        try:\n",
    "            vector[i] = models.wv[word]\n",
    "        except:\n",
    "            continue\n",
    "    target = line.split()[0]\n",
    "    if len(words) not in train_data.keys():\n",
    "        train_data[len(words)] = []\n",
    "        train_target[len(words)] = []\n",
    "    train_data[len(words)].append(vector)\n",
    "    train_target[len(words)].append(target)\n",
    "    \n",
    "print(train_data.keys())\n",
    "for key in train_data.keys():\n",
    "    train_data[key] = np.array(train_data[key])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(targets, n_target):\n",
    "    targets_set = list(set(targets))\n",
    "    results = np.zeros((len(targets), n_target))\n",
    "    for i in range(len(targets)):\n",
    "        results[i][targets_set.index(targets[i])] = 1.0\n",
    "    return results\n",
    "\n",
    "def one_hot_dict(targets, n_target):\n",
    "    results = targets\n",
    "    for key in targets.keys():\n",
    "        results[key] = one_hot(targets[key], n_target)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [i.split()[0] for i in lines]\n",
    "n_target = len(set(targets))\n",
    "train_target_one_hot = one_hot_dict(train_target, n_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5843, 13, 200)\n",
      "(5843, 134)\n",
      "(5843, 134)\n"
     ]
    }
   ],
   "source": [
    "print(train_data[13].shape)\n",
    "print(train_target[13].shape)\n",
    "print(train_target_one_hot[13].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "367e772251f3483194a77869f6c87c82"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Training Epoch 35, acc: 0.008209, err: 0.000053, n: 134000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f008e2a535e144f4b47551cb32dc0c90"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    input_placeholder = tf.placeholder(tf.float32, [None, None, 200])\n",
    "    target_placeholder = tf.placeholder(tf.float32, [None, 134])\n",
    "    \n",
    "    lstm_layer1 = lstm_layer('lstm1', input_placeholder, 200)\n",
    "    affine1 = affine_layer('affine1', lstm_layer1.outputs[:,-1,:], [200, 256], tf.nn.relu)\n",
    "    affine2 = affine_layer('affine2', affine1.outputs, [256, 256], tf.nn.relu)\n",
    "    affine3 = affine_layer('affine3', affine2.outputs, [256, 134], tf.identity)\n",
    "    \n",
    "    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=affine3.outputs, labels=target_placeholder))\n",
    "    optimizer = tf.train.AdamOptimizer(1e-3).minimize(loss)\n",
    "    accuracy = tf.reduce_sum(tf.cast(tf.equal(tf.argmax(affine3.outputs, 1), tf.argmax(target_placeholder, 1)), tf.float32))\n",
    "    \n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for i in range(500):\n",
    "        \n",
    "        accs = 0.0\n",
    "        errs = 0.0\n",
    "        n = 0\n",
    "        for key,i in zip(train_data.keys(), range(len(train_data.keys()))):\n",
    "            inputs_batch = train_data[key]\n",
    "            targets_batch = train_target_one_hot[key]\n",
    "#             print(inputs_batch.shape)\n",
    "#             print(targets_batch.shape)\n",
    "            feed_dict = {input_placeholder:inputs_batch, target_placeholder:targets_batch}\n",
    "            _, acc, err = sess.run([optimizer, accuracy, loss], feed_dict=feed_dict)\n",
    "            accs += acc\n",
    "            errs += err\n",
    "            n += len(inputs_batch)\n",
    "        print('#Training Epoch %d, acc: %f, err: %f, n: %d'%(i, accs/n, errs/n, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
