{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batched_inputs = pickle.load(open('../word2vec/batched_inputs.npz', 'rb'))\n",
    "batched_targets = pickle.load(open('../word2vec/batched_targets.npz', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class corpusClean():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.num_re = re.compile('[0-9一二三四五六七八九十]+')\n",
    "        self.char_re = re.compile('[a-zA-Z\\-\\,\\.\\。\\，]+')\n",
    "\n",
    "    def clean(self, line):\n",
    "        sentence = line\n",
    "        removed_char = re.sub(self.char_re, ' <CHAR> ', sentence)\n",
    "        removed_num = re.sub(self.num_re, ' <NUMBER> ', removed_char)\n",
    "        words = removed_num.split()\n",
    "        saved_words = ['<BEGIN>']\n",
    "        for word in words:\n",
    "            if len(word) == 1 or word.find('<') != -1:\n",
    "                saved_words.append(word)\n",
    "            else:\n",
    "                for w in word:\n",
    "                    saved_words.append(w)\n",
    "        saved_words.append('<END>')\n",
    "        return saved_words\n",
    "    \n",
    "    \n",
    "class layer(object):\n",
    "    def __init__(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def __str__(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "class affine_layer(layer):\n",
    "    def __init__(self, name, inputs, shape, activation, reuse=False):\n",
    "        with tf.variable_scope('affine', reuse=reuse):\n",
    "            self.weights = tf.get_variable(name = name+'_weights', initializer=tf.truncated_normal(shape, stddev=0.05), dtype=tf.float32)\n",
    "            self.bias = tf.get_variable(name = name+'_bias', initializer=tf.zeros([shape[-1]]))\n",
    "            self.outputs = activation(tf.add(tf.matmul(inputs, self.weights), self.bias))\n",
    "    def __str__(self):\n",
    "        return self.outputs.__str__()\n",
    "    \n",
    "class conv_layer(layer):\n",
    "    def __init__(self, name, inputs, shape, activation, reuse=False):\n",
    "        with tf.variable_scope('conv', reuse=reuse):\n",
    "            self.kernel = tf.get_variable(name = name+'_kernel', initializer=tf.truncated_normal(shape, stddev=0.05), dtype=tf.float32)\n",
    "            self.bias = tf.get_variable(name = name+'_bias', initializer=tf.zeros([shape[-1]]))\n",
    "            self.outputs = activation(tf.add(tf.nn.conv2d(inputs, self.kernel, padding='VALID', strides=[1,1,1,1]), self.bias))\n",
    "    def __str__(self):\n",
    "        return self.outputs.__str__()\n",
    "    \n",
    "class pooling_layer(layer):\n",
    "    def __init__(self, name, inputs, reuse=False):\n",
    "        with tf.variable_scope('pooling', reuse=reuse):\n",
    "            self.outputs = tf.nn.max_pool(name = name+'_maxpooling', value=inputs, ksize=[1,2,2,1], padding='VALID', strides=[1,2,2,1])\n",
    "    def __str__(self):\n",
    "        return self.outputs.__str__()\n",
    "        \n",
    "class reshape_layer(layer):\n",
    "    def __init__(self, name, inputs, shape, reuse=False):\n",
    "        with tf.variable_scope('reshape', reuse=reuse):\n",
    "            self.outputs = tf.reshape(name=name, tensor=inputs, shape=shape)\n",
    "    def __str__(self):\n",
    "        return self.outputs.__str__()\n",
    "        \n",
    "class deconv_layer(layer):\n",
    "    def __init__(self, name, inputs, kernel_shape, output_shape, activation, reuse=False):\n",
    "        with tf.variable_scope('deconv', reuse=reuse):\n",
    "            self.kernel = tf.get_variable(name = name+'_kernel', initializer=tf.truncated_normal(kernel_shape, stddev=0.05), dtype=tf.float32)\n",
    "            self.bias = tf.get_variable(name = name+'_bias', initializer=tf.zeros([kernel_shape[-2]]))\n",
    "            self.outputs = activation(tf.add(tf.nn.conv2d_transpose(inputs, self.kernel, output_shape=output_shape, padding='VALID', strides=[1,2,2,1]), self.bias))\n",
    "    def __str__(self):\n",
    "        return self.outputs.__str__()\n",
    "    \n",
    "class lstm_layer(layer):\n",
    "    def __init__(self, name, inputs, n_units, reuse=False):\n",
    "        with tf.variable_scope('lstm', reuse=reuse):\n",
    "            self.outputs, self.states = tf.nn.dynamic_rnn(\n",
    "                cell = tf.contrib.rnn.BasicLSTMCell(n_units),\n",
    "                inputs = inputs,\n",
    "                dtype = tf.float32)\n",
    "    def __str__(self):\n",
    "        return self.outputs.__str__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.42182922  0.16067666  0.0759489  -0.24928264]\n",
      " [ 0.19986795 -1.77699256 -1.39712381  0.10153597]\n",
      " [-0.1307098  -0.6939618   0.45365265  0.33374387]\n",
      " [ 0.28339007 -1.46200562  0.26699179 -1.38850904]\n",
      " [-0.61209089  0.61759257  2.35266709 -0.67915756]\n",
      " [-0.04189507 -0.4209936   0.25579578 -0.45282298]\n",
      " [-0.4843902  -2.0866611  -0.74460214  0.72786808]\n",
      " [-0.81797385 -0.50362325 -0.05660002  2.68420768]\n",
      " [-0.38786694  0.85173392  0.29009327 -0.87774098]\n",
      " [-1.14863825 -1.63646805  0.31737217  0.39797053]]\n",
      "TopKV2(values=array([[ 1.7258538 ,  1.22763908],\n",
      "       [ 1.75641167, -0.75762498],\n",
      "       [ 0.28343555, -0.46270299],\n",
      "       [ 1.16568851,  0.70551711],\n",
      "       [ 0.95638633,  0.65080678],\n",
      "       [ 0.98206872,  0.08441091],\n",
      "       [ 0.7031458 , -0.2947146 ],\n",
      "       [ 1.97574341,  1.68389535],\n",
      "       [ 0.71642655,  0.17643112],\n",
      "       [ 0.59653622,  0.06258423]], dtype=float32), indices=array([[1, 3],\n",
      "       [0, 3],\n",
      "       [3, 0],\n",
      "       [1, 3],\n",
      "       [0, 3],\n",
      "       [0, 2],\n",
      "       [3, 1],\n",
      "       [0, 3],\n",
      "       [2, 0],\n",
      "       [3, 0]]))\n"
     ]
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    input_placeholder = tf.placeholder(tf.float32, [None, None, 200])\n",
    "    target_placeholder = tf.placeholder(tf.float32, [None, 134])\n",
    "    step_size_placeholder = tf.placeholder(tf.int32)\n",
    "    batch_size_placeholder = tf.placeholder(tf.int32)\n",
    "    \n",
    "    \n",
    "    lstm_layer1 = lstm_layer('lstm1', input_placeholder, 200)\n",
    "    reshape1 = reshape_layer('reshape1', lstm_layer1.outputs, [batch_size_placeholder, step_size_placeholder*200])\n",
    "    topk1 = tf.nn.top_k(reshape1.outputs, )\n",
    "    \n",
    "    \n",
    "    affine1 = affine_layer('affine1', lstm_layer1.outputs[:,-1,:], [200, 256], tf.nn.relu)\n",
    "    affine2 = affine_layer('affine2', affine1.outputs, [256, 256], tf.nn.relu)\n",
    "    affine3 = affine_layer('affine3', affine2.outputs, [256, 134], tf.identity)\n",
    "    \n",
    "    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=affine3.outputs, labels=target_placeholder))\n",
    "    optimizer = tf.train.AdamOptimizer(1e-3).minimize(loss)\n",
    "    accuracy = tf.reduce_sum(tf.cast(tf.equal(tf.argmax(affine3.outputs, 1), tf.argmax(target_placeholder, 1)), tf.float32))\n",
    "    \n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for i in range(500):\n",
    "        \n",
    "        accs = 0.0\n",
    "        errs = 0.0\n",
    "        n = 0\n",
    "        for key in batched_inputs.keys():\n",
    "            inputs_batch = batched_inputs[key]\n",
    "            targets_batch = batched_targets[key]\n",
    "\n",
    "            feed_dict = {input_placeholder:inputs_batch, target_placeholder:targets_batch}\n",
    "            _, acc, err = sess.run([optimizer, accuracy, loss], feed_dict=feed_dict)\n",
    "            accs += acc\n",
    "            errs += err\n",
    "            n += len(inputs_batch)\n",
    "            break\n",
    "        print('#Training Epoch %d, acc: %f, err: %f, n: %d'%(i, accs/n, errs/n, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 3], [4, 5, 6]]\n"
     ]
    }
   ],
   "source": [
    "a = [[1,2,3],[4,5,6]]\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
