{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['inputs', 'targets', 'batch_size', 'units'])\n"
     ]
    }
   ],
   "source": [
    "data = pickle.load(open('../data/data.npz', 'rb'))\n",
    "print(data.keys())\n",
    "train_inputs = data['inputs']\n",
    "train_targets = data['targets']\n",
    "batch_size = data['batch_size']\n",
    "n_units = data['units']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = open('../data/cleaned_corpus.txt', encoding='utf-8').readlines()\n",
    "word_map = pickle.load(open('../data/word_map.npz', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BEGIN> 请 问 随 州 市 有 什 么 新 闻 呢 <END>\n",
      "\n",
      "[1577 1380 1624  563 1501 1756  223 1218  842  809 1682 1400 1347]\n",
      "1816\n",
      "134000\n"
     ]
    }
   ],
   "source": [
    "line = lines[0]\n",
    "print(line)\n",
    "def getIDs(line, word_map):\n",
    "    ids = []\n",
    "    for i in line.split():\n",
    "        if i not in word_map.keys():\n",
    "            ids.append(word_map['UNKNOWN'])\n",
    "        else:\n",
    "            ids.append(word_map[i])\n",
    "    return np.array(ids, dtype=np.int32)\n",
    "\n",
    "print(getIDs(line, word_map))\n",
    "print(len(word_map))\n",
    "print(len(lines))\n",
    "voc_size = len(word_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [i.split()[0] for i in open('../data/TrainSet-eCarX-171019.txt').readlines()]\n",
    "target_set = set([l for l in labels])\n",
    "target_set = list(target_set)\n",
    "# print(target_set)\n",
    "# print(len(target_set))\n",
    "target_size = len(target_set)\n",
    "train_inputs = np.array([getIDs(i, word_map) for i in lines])\n",
    "train_targets = np.zeros((len(labels), target_size))\n",
    "\n",
    "contexts = open('../data/TrainSet-eCarX-171019.txt').readlines()\n",
    "for line, i in zip(contexts, range(len(contexts))):\n",
    "    target = line.split()[0]\n",
    "    train_targets[i][target_set.index(target)] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(train_inputs))\n",
    "# print(len(train_targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class layer(object):\n",
    "    def __init__(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def __str__(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "class affine_layer(layer):\n",
    "    def __init__(self, name, inputs, shape, activation, reuse=False):\n",
    "        with tf.variable_scope('affine', reuse=reuse):\n",
    "            self.weights = tf.get_variable(name = name+'_weights', initializer=tf.truncated_normal(shape, stddev=0.05), dtype=tf.float32)\n",
    "            self.bias = tf.get_variable(name = name+'_bias', initializer=tf.zeros([shape[-1]]))\n",
    "            self.outputs = activation(tf.add(tf.matmul(inputs, self.weights), self.bias))\n",
    "    def __str__(self):\n",
    "        return self.outputs.__str__()\n",
    "    \n",
    "class conv_layer(layer):\n",
    "    def __init__(self, name, inputs, shape, activation, reuse=False):\n",
    "        with tf.variable_scope('conv', reuse=reuse):\n",
    "            self.kernel = tf.get_variable(name = name+'_kernel', initializer=tf.truncated_normal(shape, stddev=0.05), dtype=tf.float32)\n",
    "            self.bias = tf.get_variable(name = name+'_bias', initializer=tf.zeros([shape[-1]]))\n",
    "            self.outputs = activation(tf.add(tf.nn.conv2d(inputs, self.kernel, padding='VALID', strides=[1,1,1,1]), self.bias))\n",
    "    def __str__(self):\n",
    "        return self.outputs.__str__()\n",
    "    \n",
    "class pooling_layer(layer):\n",
    "    def __init__(self, name, inputs, reuse=False):\n",
    "        with tf.variable_scope('pooling', reuse=reuse):\n",
    "            self.outputs = tf.nn.max_pool(name = name+'_maxpooling', value=inputs, ksize=[1,2,2,1], padding='VALID', strides=[1,2,2,1])\n",
    "    def __str__(self):\n",
    "        return self.outputs.__str__()\n",
    "        \n",
    "class reshape_layer(layer):\n",
    "    def __init__(self, name, inputs, shape, reuse=False):\n",
    "        with tf.variable_scope('reshape', reuse=reuse):\n",
    "            self.outputs = tf.reshape(name=name, tensor=inputs, shape=shape)\n",
    "    def __str__(self):\n",
    "        return self.outputs.__str__()\n",
    "        \n",
    "class deconv_layer(layer):\n",
    "    def __init__(self, name, inputs, kernel_shape, output_shape, activation, reuse=False):\n",
    "        with tf.variable_scope('deconv', reuse=reuse):\n",
    "            self.kernel = tf.get_variable(name = name+'_kernel', initializer=tf.truncated_normal(kernel_shape, stddev=0.05), dtype=tf.float32)\n",
    "            self.bias = tf.get_variable(name = name+'_bias', initializer=tf.zeros([kernel_shape[-2]]))\n",
    "            self.outputs = activation(tf.add(tf.nn.conv2d_transpose(inputs, self.kernel, output_shape=output_shape, padding='VALID', strides=[1,2,2,1]), self.bias))\n",
    "    def __str__(self):\n",
    "        return self.outputs.__str__()\n",
    "    \n",
    "class lstm_layer(layer):\n",
    "    def __init__(self, name, inputs, n_units, reuse=False):\n",
    "        with tf.variable_scope('lstm', reuse=reuse):\n",
    "            self.outputs, self.states = tf.nn.dynamic_rnn(\n",
    "                cell = tf.contrib.rnn.BasicLSTMCell(n_units),\n",
    "                inputs = inputs,\n",
    "                dtype = tf.float32)\n",
    "    def __str__(self):\n",
    "        return self.outputs.__str__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    with tf.name_scope('embedding_lstm'):\n",
    "#     if 1 == 1:\n",
    "\n",
    "        \n",
    "        layers = {}\n",
    "        input_placeholder = tf.placeholder(tf.int32, [1, None])\n",
    "        target_placeholder = tf.placeholder(tf.float32, [1, 134])\n",
    "        layers['input_placeholder'] = input_placeholder\n",
    "        layers['target_placeholder'] = target_placeholder\n",
    "#         stepSize_placeholder = tf.placeholder(tf.int32)\n",
    "\n",
    "        voc_size = 1816\n",
    "        embedding_size = 300\n",
    "        with tf.variable_scope('embedding'):\n",
    "#         layers['embedding'] = tf.zeros([voc_size, embedding_size])\n",
    "            layers['embedding'] = tf.get_variable(name='embedding', initializer=tf.random_uniform([voc_size, embedding_size], -1.0, 1.0), dtype=tf.float32)\n",
    "        layers['input_embedding'] = tf.nn.embedding_lookup(layers['embedding'], input_placeholder)\n",
    "#         print(input_embedding)\n",
    "        with tf.name_scope('lstm_layer1'):\n",
    "            layers['lstm1'] = lstm_layer('lstm_layer1', layers['input_embedding'], embedding_size)\n",
    "#         print(lstm1)\n",
    "        with tf.name_scope('reshape_layer1'):\n",
    "            layers['reshape1'] = reshape_layer('reshape_layer1', layers['lstm1'].outputs[:,-1,:], [1, 1, embedding_size, 1])\n",
    "#         print(reshape1.outputs)\n",
    "        with tf.name_scope('conv_layer1'):\n",
    "            layers['conv1'] = conv_layer('conv_layer1', layers['reshape1'].outputs, [1, 50, 1, 1], tf.nn.relu)\n",
    "#         print(conv1.outputs)\n",
    "        with tf.name_scope('reshape_layer2'):\n",
    "            layers['reshape2'] = reshape_layer('reshape_layer2', layers['conv1'].outputs, [1, 251])\n",
    "#         print(reshape2)\n",
    "        with tf.name_scope('affine_layer1'):\n",
    "            layers['affine1'] = affine_layer('affine_layer1', layers['reshape2'].outputs, [251, 128], tf.nn.relu)\n",
    "#         print(affine1)\n",
    "        with tf.name_scope('affine_layer2'):\n",
    "            layers['out'] = affine_layer('affine_layer2', layers['affine1'].outputs, [128, 134], tf.identity)\n",
    "#         print(out)\n",
    "\n",
    "        for layer in layers.keys():\n",
    "            try:\n",
    "                print(layer.outputs)\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        sess = tf.Session()\n",
    "#         sess.run(tf.variables_initializer())\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=layers['out'].outputs, labels=target_placeholder))\n",
    "    #     print(loss)\n",
    "        accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(layers['out'].outputs, 1), tf.argmax(target_placeholder, 1)), tf.float32))\n",
    "        optimizer = tf.train.GradientDescentOptimizer(1e-3).minimize(loss)\n",
    "\n",
    "        for i in range(100):\n",
    "            errs = 0.0\n",
    "            accs = 0.0\n",
    "            n = 0\n",
    "            for inputs_batch, targets_batch in zip(train_inputs, train_targets):\n",
    "                inputs = inputs_batch.reshape(1, -1)\n",
    "                targets = targets_batch.reshape(1, -1)\n",
    "                feed_dict = {input_placeholder: inputs, target_placeholder:targets}\n",
    "                _, err, acc = sess.run([optimizer, loss, accuracy], feed_dict=feed_dict)\n",
    "                n += 1\n",
    "            errs = errs/n\n",
    "            accs = accs/n\n",
    "            print('#Training Epoch %d, ACC:%f, ERR:%f', i, accs, errs)\n",
    "\n",
    "    #     print(sess.run(input_embedding, feed_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
