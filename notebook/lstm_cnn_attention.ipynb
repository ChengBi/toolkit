{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['inputs', 'targets', 'batch_size', 'units'])\n"
     ]
    }
   ],
   "source": [
    "data = pickle.load(open('../data/data.npz', 'rb'))\n",
    "print(data.keys())\n",
    "train_inputs = data['inputs']\n",
    "train_targets = data['targets']\n",
    "batch_size = data['batch_size']\n",
    "n_units = data['units']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = open('../data/cleaned_corpus.txt', encoding='utf-8').readlines()\n",
    "word_map = pickle.load(open('../data/word_map.npz', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BEGIN> 请 问 随 州 市 有 什 么 新 闻 呢 <END>\n",
      "\n",
      "[1577 1380 1624  563 1501 1756  223 1218  842  809 1682 1400 1347]\n",
      "1816\n",
      "134000\n"
     ]
    }
   ],
   "source": [
    "line = lines[0]\n",
    "print(line)\n",
    "def getIDs(line, word_map):\n",
    "    ids = []\n",
    "    for i in line.split():\n",
    "        if i not in word_map.keys():\n",
    "            ids.append(word_map['UNKNOWN'])\n",
    "        else:\n",
    "            ids.append(word_map[i])\n",
    "    return np.array(ids, dtype=np.int32)\n",
    "print(getIDs(line, word_map))\n",
    "print(len(word_map))\n",
    "print(len(lines))\n",
    "voc_size = len(word_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [i.split()[0] for i in open('../data/TrainSet-eCarX-171019.txt').readlines()]\n",
    "target_set = set([l for l in labels])\n",
    "target_set = list(target_set)\n",
    "# print(target_set)\n",
    "# print(len(target_set))\n",
    "target_size = len(target_set)\n",
    "train_inputs = np.array([getIDs(i, word_map) for i in lines])\n",
    "train_targets = np.zeros((len(labels), target_size))\n",
    "\n",
    "contexts = open('../data/TrainSet-eCarX-171019.txt').readlines()\n",
    "for line, i in zip(contexts, range(len(contexts))):\n",
    "    target = line.split()[0]\n",
    "    train_targets[i][target_set.index(target)] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class layer(object):\n",
    "    def __init__(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def __str__(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "class affine_layer(layer):\n",
    "    def __init__(self, name, inputs, shape, activation, reuse=False):\n",
    "        with tf.variable_scope('affine', reuse=reuse):\n",
    "            self.weights = tf.get_variable(name = name+'_weights', initializer=tf.truncated_normal(shape, stddev=0.05), dtype=tf.float32)\n",
    "            self.bias = tf.get_variable(name = name+'_bias', initializer=tf.zeros([shape[-1]]))\n",
    "            self.outputs = activation(tf.add(tf.matmul(inputs, self.weights), self.bias))\n",
    "    def __str__(self):\n",
    "        return self.outputs.__str__()\n",
    "    \n",
    "class conv_layer(layer):\n",
    "    def __init__(self, name, inputs, shape, activation, reuse=False):\n",
    "        with tf.variable_scope('conv', reuse=reuse):\n",
    "            self.kernel = tf.get_variable(name = name+'_kernel', initializer=tf.truncated_normal(shape, stddev=0.05), dtype=tf.float32)\n",
    "            self.bias = tf.get_variable(name = name+'_bias', initializer=tf.zeros([shape[-1]]))\n",
    "            self.outputs = activation(tf.add(tf.nn.conv2d(inputs, self.kernel, padding='VALID', strides=[1,1,1,1]), self.bias))\n",
    "    def __str__(self):\n",
    "        return self.outputs.__str__()\n",
    "    \n",
    "class pooling_layer(layer):\n",
    "    def __init__(self, name, inputs, reuse=False):\n",
    "        with tf.variable_scope('pooling', reuse=reuse):\n",
    "            self.outputs = tf.nn.max_pool(name = name+'_maxpooling', value=inputs, ksize=[1,2,2,1], padding='VALID', strides=[1,2,2,1])\n",
    "    def __str__(self):\n",
    "        return self.outputs.__str__()\n",
    "        \n",
    "class reshape_layer(layer):\n",
    "    def __init__(self, name, inputs, shape, reuse=False):\n",
    "        with tf.variable_scope('reshape', reuse=reuse):\n",
    "            self.outputs = tf.reshape(name=name, tensor=inputs, shape=shape)\n",
    "    def __str__(self):\n",
    "        return self.outputs.__str__()\n",
    "        \n",
    "class deconv_layer(layer):\n",
    "    def __init__(self, name, inputs, kernel_shape, output_shape, activation, reuse=False):\n",
    "        with tf.variable_scope('deconv', reuse=reuse):\n",
    "            self.kernel = tf.get_variable(name = name+'_kernel', initializer=tf.truncated_normal(kernel_shape, stddev=0.05), dtype=tf.float32)\n",
    "            self.bias = tf.get_variable(name = name+'_bias', initializer=tf.zeros([kernel_shape[-2]]))\n",
    "            self.outputs = activation(tf.add(tf.nn.conv2d_transpose(inputs, self.kernel, output_shape=output_shape, padding='VALID', strides=[1,2,2,1]), self.bias))\n",
    "    def __str__(self):\n",
    "        return self.outputs.__str__()\n",
    "    \n",
    "class lstm_layer(layer):\n",
    "    def __init__(self, name, inputs, n_units, reuse=False):\n",
    "        with tf.variable_scope('lstm', reuse=reuse):\n",
    "            self.outputs, self.states = tf.nn.dynamic_rnn(\n",
    "                cell = tf.contrib.rnn.BasicLSTMCell(n_units),\n",
    "                inputs = inputs,\n",
    "                dtype = tf.float32)\n",
    "    def __str__(self):\n",
    "        return self.outputs.__str__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_inputs = dict()\n",
    "batched_targets = dict()\n",
    "batched_keys = dict()\n",
    "for input_, target_ in zip(train_inputs, train_targets):\n",
    "#     print(input_.shape)\n",
    "    if len(input_) not in batched_inputs.keys():\n",
    "        batched_inputs[len(input_)] = []\n",
    "        batched_targets[len(input_)] = []\n",
    "        batched_keys[len(input_)] = 0\n",
    "    batched_inputs[len(input_)].append(input_)\n",
    "    batched_targets[len(input_)].append(target_)\n",
    "    batched_keys[len(input_)] += 1\n",
    "#     break\n",
    "for key in batched_keys.keys():\n",
    "    batched_inputs[key] = np.array(batched_inputs[key])\n",
    "    batched_targets[key] = np.array(batched_targets[key])\n",
    "    batched_keys[key] = np.array(batched_keys[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Training Epoch 0, ACC:0.165621, ERR:0.690911\n",
      "#Training Epoch 1, ACC:0.227479, ERR:0.687589\n",
      "#Training Epoch 2, ACC:0.227493, ERR:0.684361\n",
      "#Training Epoch 3, ACC:0.227493, ERR:0.680503\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-61ce71345de4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;31m#                 print(targets.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m                 \u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0minput_placeholder\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_placeholder\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mtargets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatchSize_placeholder\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbatchSize\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m                 \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlookup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'input_embedding'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m \u001b[1;31m#                 print(lookup.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m                 \u001b[0maccs\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0macc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\cheng.bi\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    787\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 789\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    790\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\cheng.bi\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    995\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 997\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    998\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\cheng.bi\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1130\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1132\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1133\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mC:\\Users\\cheng.bi\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1137\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1138\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1139\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1140\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\cheng.bi\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1121\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    with tf.name_scope('embedding_lstm'):\n",
    "#     if 1 == 1:\n",
    "\n",
    "        \n",
    "        layers = {}\n",
    "        input_placeholder = tf.placeholder(tf.int32, [None, None])\n",
    "        target_placeholder = tf.placeholder(tf.float32, [None, 134])\n",
    "        batchSize_placeholder = tf.placeholder(tf.int32)\n",
    "        \n",
    "        layers['input_placeholder'] = input_placeholder\n",
    "        layers['target_placeholder'] = target_placeholder\n",
    "#         stepSize_placeholder = tf.placeholder(tf.int32)\n",
    "\n",
    "        voc_size = 1816\n",
    "        embedding_size = 128\n",
    "        with tf.variable_scope('embedding'):\n",
    "            layers['embedding'] = tf.get_variable(name='embedding', initializer=tf.random_uniform([voc_size, embedding_size], -1.0, 1.0), dtype=tf.float32)\n",
    "        layers['input_embedding'] = tf.nn.embedding_lookup(layers['embedding'], input_placeholder)\n",
    "\n",
    "        with tf.name_scope('lstm_layer1'):\n",
    "            layers['lstm1'] = lstm_layer('lstm_layer1', layers['input_embedding'], embedding_size)\n",
    "#         print(layers['lstm1'].outputs)\n",
    "            \n",
    "        with tf.name_scope('reshape_layer1'):\n",
    "            layers['reshape1'] = reshape_layer('reshape_layer1', layers['lstm1'].outputs[:,-1,:], [batchSize_placeholder, 1, embedding_size, 1])\n",
    "#         print(layers['reshape1'])\n",
    "        \n",
    "        with tf.name_scope('conv_layer1'):\n",
    "            layers['conv1'] = conv_layer('conv_layer1', layers['reshape1'].outputs, [1, 5, 1, 1], tf.nn.relu)\n",
    "\n",
    "        with tf.name_scope('reshape_layer2'):\n",
    "            layers['reshape2'] = reshape_layer('reshape_layer2', layers['conv1'].outputs[:,-1,:], [batchSize_placeholder, 124])\n",
    "\n",
    "        with tf.name_scope('affine_layer1'):\n",
    "            layers['affine1'] = affine_layer('affine_layer1', layers['reshape2'].outputs, [124, 124], tf.nn.relu)\n",
    "\n",
    "        with tf.name_scope('affine_layer2'):\n",
    "            layers['out'] = affine_layer('affine_layer2', layers['affine1'].outputs, [124, 134], tf.identity)\n",
    "\n",
    "\n",
    "        for layer in layers.keys():\n",
    "            try:\n",
    "                print(layer.outputs)\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        sess = tf.Session()\n",
    "#         sess.run(tf.variables_initializer())\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=layers['out'].outputs, labels=target_placeholder))\n",
    "    #     print(loss)\n",
    "        accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(layers['out'].outputs, 1), tf.argmax(target_placeholder, 1)), tf.float32))\n",
    "        optimizer = tf.train.GradientDescentOptimizer(5e-2).minimize(loss)\n",
    "\n",
    "        for i in range(100):\n",
    "            errs = 0.0\n",
    "            accs = 0.0\n",
    "            n = 0\n",
    "            for key in batched_keys.keys():\n",
    "#                 inputs = inputs_batch.reshape(1, -1)\n",
    "#                 targets = targets_batch.reshape(1, -1)\n",
    "                inputs = batched_inputs[key]\n",
    "                targets = batched_targets[key]\n",
    "                batchSize = batched_keys[key]\n",
    "#                 print(batchSize)\n",
    "#                 print(inputs.shape)\n",
    "#                 print(targets.shape)\n",
    "                feed_dict = {input_placeholder: inputs, target_placeholder:targets, batchSize_placeholder:batchSize}\n",
    "                _, err, acc, lookup = sess.run([optimizer, loss, accuracy, layers['input_embedding']], feed_dict=feed_dict)\n",
    "#                 print(lookup.shape)\n",
    "                accs += acc\n",
    "                errs += err\n",
    "                n += 1\n",
    "            errs = errs/n\n",
    "            accs = accs/n\n",
    "            print('#Training Epoch %d, ACC:%f, ERR:%f'%(i, accs, errs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
